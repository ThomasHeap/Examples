{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasHeap/Examples/blob/main/M2L_summer_school/NLP/part_II_text_generation/Transformer_Decoder_MoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing Tutorial\n",
        "======\n",
        "\n",
        "This is the tutorial of the 2024 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n",
        "\n",
        "This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Basic Python programming skills are expected.\n",
        "Prior knowledge of standard NLP techniques (e.g. text tokenization and classification with ML) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n",
        "\n",
        "This tutorial combines detailed analysis and development of essential NLP concepts via custom (i.e. from scratch) implementations. Other necessary NLP components will be developed using PyTorch's NLP library implementations. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications.\n",
        "\n",
        "## Outline\n",
        "\n",
        "* Part I: Introduction to Text Tokenization and Classification\n",
        "  *  Text Classification: Simple Classifier\n",
        "  *  Text Classification: Encoder-only Transformer\n",
        "\n",
        "* Part II: Introduction to Decoder-only Transformer and Sparse Mixture of Experts Architecture\n",
        "  *  Text Generation: Decoder-only Transformer\n",
        "  *  Text Generation: Decoder-only Transformer + MoE\n",
        "\n",
        "* Part III: Introduction to Parameter Efficient Fine-tuning\n",
        "  *  Fine-tuning the full Pre-trained Models\n",
        "  *  Fine-tuning using Low-Rank Adaptation of Large Language Models (LoRA)\n",
        "\n",
        "## Notation\n",
        "\n",
        "* Sections marked as [📝] contain cells with missing code that you should complete.\n",
        "* Sections marked with [📚] contain cells that you should read and modify to understand how your changes alter the obtained results.\n",
        "* External resources are mentioned with [✨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered.\n",
        "* Sections that contain code that test the functionality of other sections are marked with [✍]. You are more that welcome to modify these sections so that you can understand code functionality.\n",
        "\n",
        "\n",
        "## Libraries\n",
        "\n",
        "This tutorial leverages [PyTorch](https://pytorch.org/) for neural network implementation and training, complemented by standard Python libraries for data processing and the [Hugging Face](https://huggingface.co/) datasets library for accessing NLP resources.\n",
        "\n",
        "GPU access is recommended for optimal performance, particularly for model training and text generation. While all code can run on CPU, a CUDA-enabled environment will significantly speed up these processes.\n",
        "\n",
        "## Credits\n",
        "\n",
        "The tutorial is created by:\n",
        "\n",
        "* [Georgios Peikos](https://www.linkedin.com/in/peikosgeorgios/)\n",
        "* [Luca Herranz-Celotti](http://LuCeHe.github.io)\n",
        "\n",
        "It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading.\n",
        "\n",
        "## Note for Colab users\n",
        "\n",
        "To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`\n",
        "\n"
      ],
      "metadata": {
        "id": "KBrDjSR61FHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nhQDxGSyOQh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Introduction to the decoder-only Transformers architecture and Sparse Mixture of Expert\n",
        "\n",
        "We create a decoder-only Transformer architecture from the bottom up, including a custom text tokenizer and an efficient dataset handler. We will explore all essential components of this architecture, train the model, and show its capabilities in text generation.\n",
        "\n",
        "Then, we will enhance our base model by incorporating a gating function and implementing a sparse mixture of experts."
      ],
      "metadata": {
        "id": "cCNKdaEAOFeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7wFJ2UqTOcpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder-only Transformer Architecture\n",
        "\n",
        "The decoder-only transformer architecture consists of multiple identical blocks stacked sequentially. Each block is composed of two main elements:\n",
        "- A masked multi-head self-attention mechanism.\n",
        "- A feed-forward neural network.\n",
        "\n",
        "These components are typically encapsulated within residual connections and layer normalization. In this section, we will explore the internal structure of these blocks in greater depth and provide a practical PyTorch implementation."
      ],
      "metadata": {
        "id": "bI8usToL1SAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V1Z2GWVTShRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UJPlhQquMr9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "cSQDU10O6YUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Union\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "E532K_c76aIB",
        "outputId": "3bfee4b4-579d-477b-9868-151c0faa90f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.5.0\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Text Tokenization from Scratch\n",
        "\n",
        "Tokenization is a fundamental step in NLP that converts raw text into a format that systems can understand and process. It enables the transformation of variable-length text sequences into fixed-size numerical representations, which is crucial for input to neural network models.\n",
        "\n",
        "Here, we create a simple text tokenizer for basic word-level tokenization tasks.\n",
        "The tokenizer can be improved so that:\n",
        "\n",
        "1.   Methods for handling very large vocabularies (e.g., frequency thresholding)\n",
        "2.   Support for n-grams or phrase detection\n",
        "3.   Handle punctuation. For instance now, tokens like \"word.\" and \"word\" are being treated differently.\n",
        "\n",
        "Also, we create a testing function to showcase the codes behavior.\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   Overview of hugging Face tokenizers [Link-huggingface](https://huggingface.co/docs/transformers/en/tokenizer_summary)"
      ],
      "metadata": {
        "id": "HNcnHkLd6Vab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the tokenizer with special tokens and prepare vocabulary structures.\"\"\"\n",
        "        # Special tokens are used for various purposes in NLP tasks:\n",
        "        # <PAD>: Used for padding sequences to a fixed length\n",
        "        # <UNK>: Represents unknown words not in the vocabulary\n",
        "        # <SOS>: Marks the start of a sequence\n",
        "        # <EOS>: Marks the end of a sequence\n",
        "        self.special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
        "\n",
        "        # word_to_idx: Maps words to unique integer indices\n",
        "        # This is crucial for converting text into a format that neural networks can process\n",
        "        self.word_to_idx = {token: idx for idx, token in enumerate(self.special_tokens)}\n",
        "\n",
        "        # idx_to_word: The reverse mapping of word_to_idx\n",
        "        # This is used for converting model outputs back into readable text\n",
        "        self.idx_to_word = {idx: token for idx, token in enumerate(self.special_tokens)}\n",
        "\n",
        "        # Counter object to keep track of word frequencies in the corpus\n",
        "        self.word_count = Counter()\n",
        "\n",
        "    def fit(self, texts: List[str]) -> None:\n",
        "        \"\"\"Build the vocabulary from a list of texts.\"\"\"\n",
        "        # Count the frequency of each word in the entire corpus\n",
        "        for text in texts:\n",
        "            self.word_count.update(text.split())\n",
        "\n",
        "        # Add each unique word to the vocabulary\n",
        "        # We assign a unique index to each word, which the model will use to represent words\n",
        "        for word in self.word_count:\n",
        "            if word not in self.word_to_idx:\n",
        "                idx = len(self.word_to_idx)\n",
        "                self.word_to_idx[word] = idx\n",
        "                self.idx_to_word[idx] = word\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert a text string to a list of indices.\"\"\"\n",
        "        # This method is used to prepare input for the model\n",
        "        # It converts each word to its corresponding index\n",
        "        # If a word is not in the vocabulary, it uses the <UNK> token\n",
        "        return [self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]) for word in text.split()]\n",
        "\n",
        "    def decode(self, indices: List[int]) -> str:\n",
        "        \"\"\"Convert a list of indices back to a text string.\"\"\"\n",
        "        # This method is used to convert model output back into readable text\n",
        "        # It maps each index back to its corresponding word\n",
        "        return \" \".join([self.idx_to_word.get(idx, \"<UNK>\") for idx in indices])\n",
        "\n",
        "    def encode_batch(self, texts: List[str]) -> List[List[int]]:\n",
        "        \"\"\"Convert a batch of text strings to lists of indices.\"\"\"\n",
        "        return [self.encode(text) for text in texts]\n",
        "\n",
        "    def decode_batch(self, batch_indices: List[List[int]]) -> List[str]:\n",
        "        \"\"\"Convert a batch of lists of indices back to text strings.\"\"\"\n",
        "        return [self.decode(indices) for indices in batch_indices]\n",
        "\n",
        "    def show_vocab(self):\n",
        "        \"\"\"Display the vocabulary.\"\"\"\n",
        "        # Useful for debugging and understanding the tokenizer's state\n",
        "        print(\"Vocabulary:\")\n",
        "        for word, idx in self.word_to_idx.items():\n",
        "            print(f\"{word}: {idx}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
        "        # The vocabulary size is an important parameter for the model\n",
        "        # It determines the dimensionality of the model's output layer\n",
        "        return len(self.word_to_idx)"
      ],
      "metadata": {
        "id": "mZ6q24hC1Q2J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Tokenizer\n",
        "\n",
        "This testing function shows examples of text tokenization presenting also extreme use cases."
      ],
      "metadata": {
        "id": "V8FKgjcjZyj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenizer():\n",
        "    print(\"\\nTesting SimpleTokenizer\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Sample texts\n",
        "    texts = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Pack my box with five dozen liquor jugs!\",\n",
        "        \"How vexingly quick daft zebras jump!\",\n",
        "        \"This is a sentence with some punctuation, including commas.\",\n",
        "        \"This text contains an unknown word: monkey\",\n",
        "        \"\"  # Empty string to test edge case\n",
        "    ]\n",
        "\n",
        "    # Initialize and fit the tokenizer\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    tokenizer.fit(texts)\n",
        "\n",
        "    # Display vocabulary\n",
        "    tokenizer.show_vocab()\n",
        "    print(f\"\\nVocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "    # Test encoding and decoding\n",
        "    print(\"\\nEncoding and Decoding Test:\")\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode(text)\n",
        "        decoded = tokenizer.decode(encoded)\n",
        "        print(f\"\\nOriginal: {text}\")\n",
        "        print(f\"Encoded : {encoded}\")\n",
        "        print(f\"Decoded : {decoded}\")\n",
        "        print(f\"Match   : {'✓' if text.strip().lower() == decoded.strip().lower() else '✗'}\")\n",
        "\n",
        "    # Test unknown word handling\n",
        "    print(\"\\nUnknown Word Handling Test:\")\n",
        "    unknown_text = \"This text contains an unknown word: xylophone\"\n",
        "    encoded_unknown = tokenizer.encode(unknown_text)\n",
        "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
        "    print(f\"Original: {unknown_text}\")\n",
        "    print(f\"Encoded : {encoded_unknown}\")\n",
        "    print(f\"Decoded : {decoded_unknown}\")\n",
        "\n",
        "    # Test special tokens\n",
        "    print(\"\\nSpecial Tokens Test:\")\n",
        "    special_text = \"< SOS > This is a test sentence <EOS>\"\n",
        "    encoded_special = tokenizer.encode(special_text)\n",
        "    decoded_special = tokenizer.decode(encoded_special)\n",
        "    print(f\"Original: {special_text}\")\n",
        "    print(f\"Encoded : {encoded_special}\")\n",
        "    print(f\"Decoded : {decoded_special}\")\n",
        "\n",
        "    # Test case sensitivity\n",
        "    print(\"\\nCase Sensitivity Test:\")\n",
        "    case_text = \"The Quick Brown Fox\"\n",
        "    encoded_case = tokenizer.encode(case_text)\n",
        "    decoded_case = tokenizer.decode(encoded_case)\n",
        "    print(f\"Original: {case_text}\")\n",
        "    print(f\"Encoded : {encoded_case}\")\n",
        "    print(f\"Decoded : {decoded_case}\")\n",
        "\n",
        "print(\"\\nChecking the tokenizer's outputs\")\n",
        "test_tokenizer()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tc3YteacXJOs",
        "outputId": "e76f03e3-70b4-483d-9f11-395a18050b7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking the tokenizer's outputs\n",
            "\n",
            "Testing SimpleTokenizer\n",
            "==============================\n",
            "Vocabulary:\n",
            "<PAD>: 0\n",
            "<UNK>: 1\n",
            "<SOS>: 2\n",
            "<EOS>: 3\n",
            "The: 4\n",
            "quick: 5\n",
            "brown: 6\n",
            "fox: 7\n",
            "jumps: 8\n",
            "over: 9\n",
            "the: 10\n",
            "lazy: 11\n",
            "dog.: 12\n",
            "Pack: 13\n",
            "my: 14\n",
            "box: 15\n",
            "with: 16\n",
            "five: 17\n",
            "dozen: 18\n",
            "liquor: 19\n",
            "jugs!: 20\n",
            "How: 21\n",
            "vexingly: 22\n",
            "daft: 23\n",
            "zebras: 24\n",
            "jump!: 25\n",
            "This: 26\n",
            "is: 27\n",
            "a: 28\n",
            "sentence: 29\n",
            "some: 30\n",
            "punctuation,: 31\n",
            "including: 32\n",
            "commas.: 33\n",
            "text: 34\n",
            "contains: 35\n",
            "an: 36\n",
            "unknown: 37\n",
            "word:: 38\n",
            "monkey: 39\n",
            "\n",
            "Vocabulary size: 40\n",
            "\n",
            "Encoding and Decoding Test:\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Encoded : [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "Decoded : The quick brown fox jumps over the lazy dog.\n",
            "Match   : ✓\n",
            "\n",
            "Original: Pack my box with five dozen liquor jugs!\n",
            "Encoded : [13, 14, 15, 16, 17, 18, 19, 20]\n",
            "Decoded : Pack my box with five dozen liquor jugs!\n",
            "Match   : ✓\n",
            "\n",
            "Original: How vexingly quick daft zebras jump!\n",
            "Encoded : [21, 22, 5, 23, 24, 25]\n",
            "Decoded : How vexingly quick daft zebras jump!\n",
            "Match   : ✓\n",
            "\n",
            "Original: This is a sentence with some punctuation, including commas.\n",
            "Encoded : [26, 27, 28, 29, 16, 30, 31, 32, 33]\n",
            "Decoded : This is a sentence with some punctuation, including commas.\n",
            "Match   : ✓\n",
            "\n",
            "Original: This text contains an unknown word: monkey\n",
            "Encoded : [26, 34, 35, 36, 37, 38, 39]\n",
            "Decoded : This text contains an unknown word: monkey\n",
            "Match   : ✓\n",
            "\n",
            "Original: \n",
            "Encoded : []\n",
            "Decoded : \n",
            "Match   : ✓\n",
            "\n",
            "Unknown Word Handling Test:\n",
            "Original: This text contains an unknown word: xylophone\n",
            "Encoded : [26, 34, 35, 36, 37, 38, 1]\n",
            "Decoded : This text contains an unknown word: <UNK>\n",
            "\n",
            "Special Tokens Test:\n",
            "Original: < SOS > This is a test sentence <EOS>\n",
            "Encoded : [1, 1, 1, 26, 27, 28, 1, 29, 3]\n",
            "Decoded : <UNK> <UNK> <UNK> This is a <UNK> sentence <EOS>\n",
            "\n",
            "Case Sensitivity Test:\n",
            "Original: The Quick Brown Fox\n",
            "Encoded : [4, 1, 1, 1]\n",
            "Decoded : The <UNK> <UNK> <UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 TextDataset: Efficient Text Processing\n",
        "\n",
        "The TextDataset class is a crucial component in preparing text data for deep learning models, implementing a sliding window approach that allows processing of variable-length texts while maintaining context.\n",
        "\n",
        "This class bridges the gap between raw text data and the input requirements of neural networks, handling tasks such as tokenization, padding, and attention mask generation, which are essential for training effective sequence models like Transformers.\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   Padding and truncation [Link-huggingface](https://huggingface.co/docs/transformers/en/pad_truncation)\n"
      ],
      "metadata": {
        "id": "JS3le4cC6do5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer: SimpleTokenizer, max_length: int, overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize the TextDataset with sliding window functionality.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of input texts.\n",
        "            tokenizer (SimpleTokenizer): Tokenizer object for encoding texts.\n",
        "            max_length (int): Maximum length of encoded sequences.\n",
        "            overlap (int): Number of overlapping tokens between windows.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.overlap = overlap\n",
        "        self.data = []\n",
        "        self.attention_masks = []\n",
        "        self.document_map = []  # Maps each window to its original document\n",
        "        self.original_texts = texts  # Store original texts\n",
        "\n",
        "        for doc_idx, text in enumerate(texts):\n",
        "            tokens = self.tokenizer.encode(text)\n",
        "            windows = self.create_sliding_windows(tokens)\n",
        "\n",
        "            for window in windows:\n",
        "                attention_mask = [1] * len(window)  # 1 for real tokens\n",
        "\n",
        "                # Pad if necessary\n",
        "                if len(window) < max_length:\n",
        "                    padding_length = max_length - len(window)\n",
        "                    window = window + [self.tokenizer.word_to_idx[\"<PAD>\"]] * padding_length\n",
        "                    attention_mask = attention_mask + [0] * padding_length  # 0 for padding in attention mask\n",
        "\n",
        "                self.data.append(window)\n",
        "                self.attention_masks.append(attention_mask)\n",
        "                self.document_map.append(doc_idx)\n",
        "\n",
        "    def create_sliding_windows(self, tokens: List[int]) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Create sliding windows from a list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens (List[int]): List of token ids.\n",
        "\n",
        "        Returns:\n",
        "            List[List[int]]: List of token windows.\n",
        "        \"\"\"\n",
        "        windows = []\n",
        "        # Calculate stride: how many tokens to move for each new window\n",
        "        # -1 accounts for the added <SOS> token at the start of each window\n",
        "        stride = self.max_length - self.overlap - 1\n",
        "\n",
        "        for start in range(0, len(tokens), stride):\n",
        "            # Create a window starting with <SOS> token\n",
        "            window = [self.tokenizer.word_to_idx[\"<SOS>\"]] + tokens[start:start + self.max_length - 1]\n",
        "            if len(window) < self.max_length:\n",
        "                # This is the last window, add <EOS> token\n",
        "                window.append(self.tokenizer.word_to_idx[\"<EOS>\"])\n",
        "            windows.append(window)\n",
        "\n",
        "        return windows\n",
        "\n",
        "    def get_original_document(self, doc_idx: int) -> str:\n",
        "        \"\"\"Retrieve the original document text.\"\"\"\n",
        "        if 0 <= doc_idx < len(self.original_texts):\n",
        "            return self.original_texts[doc_idx]\n",
        "        else:\n",
        "            raise IndexError(f\"Document index {doc_idx} is out of range.\")\n",
        "\n",
        "    def get_document_length(self, doc_idx: int) -> int:\n",
        "        \"\"\"Get the number of tokens in the original document.\"\"\"\n",
        "        if 0 <= doc_idx < len(self.original_texts):\n",
        "            return len(self.tokenizer.encode(self.original_texts[doc_idx]))\n",
        "        else:\n",
        "            raise IndexError(f\"Document index {doc_idx} is out of range.\")\n",
        "\n",
        "    def window_to_document_position(self, window_idx: int, token_idx: int) -> Tuple[int, int]:\n",
        "        \"\"\"Map a position in a window back to its position in the original document.\"\"\"\n",
        "        if 0 <= window_idx < len(self.data):\n",
        "            doc_idx = self.document_map[window_idx]\n",
        "            doc_windows = self.get_document_windows(doc_idx)\n",
        "            # Find which window of the document this is\n",
        "            relative_window_idx = doc_windows.index(window_idx)\n",
        "            # Calculate the start position of this window in the document\n",
        "            window_start = relative_window_idx * (self.max_length - self.overlap - 1)\n",
        "            # -1 to account for <SOS> token at the start of each window\n",
        "            return doc_idx, window_start + token_idx - 1\n",
        "        else:\n",
        "            raise IndexError(f\"Window index {window_idx} is out of range.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Get the number of windows in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
        "        \"\"\"\n",
        "        Get a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, int]:\n",
        "                A tuple containing (token_ids, attention_mask, document_index).\n",
        "        \"\"\"\n",
        "        if 0 <= idx < len(self.data):\n",
        "            # Add an extra dimension to make it batch-first (batch_size=1)\n",
        "            return (torch.tensor(self.data[idx]).unsqueeze(0),\n",
        "                    torch.tensor(self.attention_masks[idx]).unsqueeze(0),\n",
        "                    self.document_map[idx])\n",
        "        else:\n",
        "            raise IndexError(f\"Index {idx} is out of range.\")\n",
        "\n",
        "\n",
        "    def get_document_windows(self, doc_idx: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Get all window indices for a specific document.\n",
        "\n",
        "        Args:\n",
        "            doc_idx (int): Index of the document.\n",
        "\n",
        "        Returns:\n",
        "            List[int]: List of window indices belonging to the document.\n",
        "        \"\"\"\n",
        "        return [i for i, doc in enumerate(self.document_map) if doc == doc_idx]"
      ],
      "metadata": {
        "id": "CeBCUMY16jms"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Dataset Processing\n",
        "\n",
        "This testing function shows how the TextDataset and SimpleTokenizer classes work together."
      ],
      "metadata": {
        "id": "QMa3_yaQgAYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sliding_window_dataset():\n",
        "    print(\"\\n--- Testing Sliding Window Dataset ---\\n\")\n",
        "\n",
        "    texts = [\n",
        "        \"This is a short sentence.\",\n",
        "        \"This is a much longer sentence that will be split into multiple windows to demonstrate the sliding window approach. It contains enough tokens to create at least two or three windows depending on the chosen maximum length and overlap.\",\n",
        "        \"Another sentence of medium length that might create two windows.\",\n",
        "        \"\",  # Empty text to test edge case\n",
        "        \"Short.\"  # Very short text to test edge case\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        tokenizer = SimpleTokenizer()\n",
        "        tokenizer.fit(texts)\n",
        "\n",
        "        max_length = 16\n",
        "        overlap = 5\n",
        "        dataset = TextDataset(texts, tokenizer, max_length, overlap)\n",
        "\n",
        "        print(f\"Dataset configuration:\")\n",
        "        print(f\"  Max length: {max_length}\")\n",
        "        print(f\"  Overlap: {overlap}\")\n",
        "        print(f\"  Total windows: {len(dataset)}\")\n",
        "        print(f\"  Vocabulary size: {len(tokenizer)}\\n\")\n",
        "\n",
        "        for doc_idx, text in enumerate(texts):\n",
        "            print(f\"Document {doc_idx}:\")\n",
        "            print(f\"  Original text: '{text}'\")\n",
        "            print(f\"  Original length: {len(text.split())}\")\n",
        "\n",
        "            window_indices = dataset.get_document_windows(doc_idx)\n",
        "            print(f\"  Number of windows: {len(window_indices)}\")\n",
        "\n",
        "            for i, window_idx in enumerate(window_indices):\n",
        "                tokens, attention_mask, _ = dataset[window_idx]\n",
        "                # Remove the batch dimension for decoding\n",
        "                decoded = tokenizer.decode(tokens.squeeze(0).tolist())\n",
        "                print(f\"\\n    Window {i}:\")\n",
        "                print(f\"    Tokens shape: {tokens.shape}\")\n",
        "                print(f\"    Tokens: {tokens.squeeze(0).tolist()}\")\n",
        "                print(f\"    Attention mask shape: {attention_mask.shape}\")\n",
        "                print(f\"    Attention mask: {attention_mask.squeeze(0).tolist()}\")\n",
        "                print(f\"    Decoded: '{decoded}'\")\n",
        "                print(f\"    Window length: {tokens.size(1)}\")  # Use size(1) for sequence length\n",
        "\n",
        "            print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "        tokenizer.show_vocab()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    print(\"\\n--- End of Test ---\")\n",
        "\n",
        "# Run the test\n",
        "test_sliding_window_dataset()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AwEtTxasgHC4",
        "outputId": "208c3dbb-1bfb-4297-9e1a-64e8630ca16c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Sliding Window Dataset ---\n",
            "\n",
            "Dataset configuration:\n",
            "  Max length: 16\n",
            "  Overlap: 5\n",
            "  Total windows: 7\n",
            "  Vocabulary size: 48\n",
            "\n",
            "Document 0:\n",
            "  Original text: 'This is a short sentence.'\n",
            "  Original length: 5\n",
            "  Number of windows: 1\n",
            "\n",
            "    Window 0:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 4, 5, 6, 7, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    Decoded: '<SOS> This is a short sentence. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "    Window length: 16\n",
            "\n",
            "--------------------------------------------------\n",
            "Document 1:\n",
            "  Original text: 'This is a much longer sentence that will be split into multiple windows to demonstrate the sliding window approach. It contains enough tokens to create at least two or three windows depending on the chosen maximum length and overlap.'\n",
            "  Original length: 39\n",
            "  Number of windows: 4\n",
            "\n",
            "    Window 0:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    Decoded: '<SOS> This is a much longer sentence that will be split into multiple windows to demonstrate'\n",
            "    Window length: 16\n",
            "\n",
            "    Window 1:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 19, 29]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    Decoded: '<SOS> into multiple windows to demonstrate the sliding window approach. It contains enough tokens to create'\n",
            "    Window length: 16\n",
            "\n",
            "    Window 2:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 26, 27, 28, 19, 29, 30, 31, 32, 33, 34, 18, 35, 36, 21, 37]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    Decoded: '<SOS> contains enough tokens to create at least two or three windows depending on the chosen'\n",
            "    Window length: 16\n",
            "\n",
            "    Window 3:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 18, 35, 36, 21, 37, 38, 39, 40, 41, 3, 0, 0, 0, 0, 0]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "    Decoded: '<SOS> windows depending on the chosen maximum length and overlap. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "    Window length: 16\n",
            "\n",
            "--------------------------------------------------\n",
            "Document 2:\n",
            "  Original text: 'Another sentence of medium length that might create two windows.'\n",
            "  Original length: 10\n",
            "  Number of windows: 1\n",
            "\n",
            "    Window 0:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 42, 11, 43, 44, 39, 12, 45, 29, 32, 46, 3, 0, 0, 0, 0]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "    Decoded: '<SOS> Another sentence of medium length that might create two windows. <EOS> <PAD> <PAD> <PAD> <PAD>'\n",
            "    Window length: 16\n",
            "\n",
            "--------------------------------------------------\n",
            "Document 3:\n",
            "  Original text: ''\n",
            "  Original length: 0\n",
            "  Number of windows: 0\n",
            "\n",
            "--------------------------------------------------\n",
            "Document 4:\n",
            "  Original text: 'Short.'\n",
            "  Original length: 1\n",
            "  Number of windows: 1\n",
            "\n",
            "    Window 0:\n",
            "    Tokens shape: torch.Size([1, 16])\n",
            "    Tokens: [2, 47, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    Attention mask shape: torch.Size([1, 16])\n",
            "    Attention mask: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    Decoded: '<SOS> Short. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "    Window length: 16\n",
            "\n",
            "--------------------------------------------------\n",
            "Vocabulary:\n",
            "<PAD>: 0\n",
            "<UNK>: 1\n",
            "<SOS>: 2\n",
            "<EOS>: 3\n",
            "This: 4\n",
            "is: 5\n",
            "a: 6\n",
            "short: 7\n",
            "sentence.: 8\n",
            "much: 9\n",
            "longer: 10\n",
            "sentence: 11\n",
            "that: 12\n",
            "will: 13\n",
            "be: 14\n",
            "split: 15\n",
            "into: 16\n",
            "multiple: 17\n",
            "windows: 18\n",
            "to: 19\n",
            "demonstrate: 20\n",
            "the: 21\n",
            "sliding: 22\n",
            "window: 23\n",
            "approach.: 24\n",
            "It: 25\n",
            "contains: 26\n",
            "enough: 27\n",
            "tokens: 28\n",
            "create: 29\n",
            "at: 30\n",
            "least: 31\n",
            "two: 32\n",
            "or: 33\n",
            "three: 34\n",
            "depending: 35\n",
            "on: 36\n",
            "chosen: 37\n",
            "maximum: 38\n",
            "length: 39\n",
            "and: 40\n",
            "overlap.: 41\n",
            "Another: 42\n",
            "of: 43\n",
            "medium: 44\n",
            "might: 45\n",
            "windows.: 46\n",
            "Short.: 47\n",
            "\n",
            "--- End of Test ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Positional Encoding\n",
        "\n",
        "Positional Encoding adds information about the position of each token in the sequence. This is necessary because the self-attention mechanism in Transformers doesn't inherently have a notion of token order.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   Transformer Architecture: The Positional Encoding [Link-kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
        "\n",
        "*   Positional Encoding in Transformers [Link-geeksforgeeks](https://www.geeksforgeeks.org/positional-encoding-in-transformers/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BHKvbddC6lpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        #######################Code Here###############################\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        ## pe = ...\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Compute the `div_term` for the frequency component using the formula involving d_model\n",
        "        ## div_term =\n",
        "        div_term = 10000 ** (torch.arange(0, d_model, 2, dtype=torch.float) / d_model)\n",
        "\n",
        "        # Populate the even indices (0, 2, 4, ...) of `pe` with the sine of the position multiplied by the `div_term`\n",
        "        ## pe[:, 0::2] =\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "\n",
        "        # Populate the odd indices (1, 3, 5, ...) of `pe` with the cosine of the position multiplied by the `div_term`\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "\n",
        "        # Add an *extra dimension* to `pe` to fit the batch dimension requirements\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register `pe` as a buffer to ensure it is not considered a model parameter but is still moved to the appropriate device when the model is moved\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "9owPpdm-neKA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing your Positional Encoding Function"
      ],
      "metadata": {
        "id": "mLYaklgz6bq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding_consistency():\n",
        "    # Test parameters\n",
        "    d_model = 4\n",
        "    max_len = 10\n",
        "    batch_size = 2\n",
        "    seq_len = 3\n",
        "\n",
        "    # Initialize a new PositionalEncoding module each time\n",
        "    pe = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "    # Create a small input tensor\n",
        "    x1 = torch.zeros(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Run the positional encoding\n",
        "    output = pe(x1)\n",
        "\n",
        "    print(\"Input:\")\n",
        "    print(x1)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    print(\"\"\"\\nNote: If your\n",
        "    Output:\n",
        "        tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
        "                [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
        "                [ 0.9093, -0.4161,  0.0200,  0.9998]],\n",
        "\n",
        "                [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
        "                [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
        "                [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
        "             , the PositionalEncoding is consistent.\"\"\")\n",
        "\n",
        "test_positional_encoding_consistency()"
      ],
      "metadata": {
        "id": "LfSwCfMy6gZH",
        "outputId": "ed03b0cb-8d05-4414-d782-afc1af9a9bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "\n",
            "Output:\n",
            "tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "         [ 0.9093, -0.4161,  0.0200,  0.9998]],\n",
            "\n",
            "        [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "         [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
            "\n",
            "Note: If your\n",
            "    Output:\n",
            "        tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "                [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "                [ 0.9093, -0.4161,  0.0200,  0.9998]],\n",
            "\n",
            "                [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "                [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "                [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
            "             , the PositionalEncoding is consistent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Positional Encoding for Text"
      ],
      "metadata": {
        "id": "vcOi9vZdmc1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding_with_dataset():\n",
        "    print(\"\\n--- Testing Positional Encoding with Dataset ---\\n\")\n",
        "\n",
        "    # Set a fixed seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Sample texts\n",
        "    texts = [\n",
        "        \"This is a short sentence.\",\n",
        "        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n",
        "        \"Another sentence of medium length.\"\n",
        "    ]\n",
        "\n",
        "    # Initialize tokenizer and fit it to the texts\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    tokenizer.fit(texts)\n",
        "\n",
        "    # Create dataset\n",
        "    max_length = 10\n",
        "    overlap = 2\n",
        "    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n",
        "\n",
        "    # Initialize positional encoding\n",
        "    d_model = 16  # Small dimension for demonstration\n",
        "    pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "    print(f\"Dataset configuration:\")\n",
        "    print(f\"  Max length: {max_length}\")\n",
        "    print(f\"  Overlap: {overlap}\")\n",
        "    print(f\"  Total windows: {len(dataset)}\")\n",
        "    print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
        "    print(f\"  Embedding dimension: {d_model}\\n\")\n",
        "\n",
        "    # Process each window through the positional encoding\n",
        "    for i in range(len(dataset)):\n",
        "        tokens, attention_mask, doc_idx = dataset[i]\n",
        "\n",
        "        # Convert tokens to \"embeddings\" (just for demonstration)\n",
        "        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        encoded = pos_encoder(pseudo_embeddings)\n",
        "\n",
        "        print(f\"Window {i} (from document {doc_idx}):\")\n",
        "        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n",
        "        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n",
        "        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n",
        "        print(f\"  Shape after positional encoding: {encoded.shape}\")\n",
        "\n",
        "        # Display the positional encoding effect for all tokens\n",
        "        print(f\"  Positional encoding effect:\")\n",
        "        for j in range(tokens.size(1)):\n",
        "            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n",
        "                print(f\"    Token {j}:\")\n",
        "                print(f\"      Before: {pseudo_embeddings[0, j, :].tolist()}\")\n",
        "                print(f\"      After:  {encoded[0, j, :].tolist()}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"--- End of Test ---\")\n",
        "\n",
        "# Run the test\n",
        "test_positional_encoding_with_dataset()"
      ],
      "metadata": {
        "id": "jOzH22PBmfyG",
        "outputId": "dd99d547-ebcd-40f4-b3a6-586c28079beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Positional Encoding with Dataset ---\n",
            "\n",
            "Dataset configuration:\n",
            "  Max length: 10\n",
            "  Overlap: 2\n",
            "  Total windows: 5\n",
            "  Vocabulary size: 27\n",
            "  Embedding dimension: 16\n",
            "\n",
            "Window 0 (from document 0):\n",
            "  Original tokens: [2, 4, 5, 6, 7, 8, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> This is a short sentence. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Positional encoding effect:\n",
            "    Token 0:\n",
            "      Before: [0.8822692632675171, 0.9150039553642273, 0.38286375999450684, 0.9593056440353394, 0.3904482126235962, 0.600895345211029, 0.2565724849700928, 0.7936413288116455, 0.9407714605331421, 0.13318592309951782, 0.9345980882644653, 0.5935796499252319, 0.8694044351577759, 0.5677152872085571, 0.7410940527915955, 0.42940449714660645]\n",
            "      After:  [0.8822692632675171, 1.915004014968872, 0.38286375999450684, 1.9593056440353394, 0.3904482126235962, 1.6008954048156738, 0.2565724849700928, 1.7936413288116455, 0.9407714605331421, 1.133185863494873, 0.9345980882644653, 1.593579649925232, 0.8694044351577759, 1.5677152872085571, 0.7410940527915955, 1.4294044971466064]\n",
            "    Token 1:\n",
            "      Before: [0.8854429125785828, 0.5739044547080994, 0.2665800452232361, 0.6274491548538208, 0.26963168382644653, 0.4413635730743408, 0.2969208359718323, 0.831685483455658, 0.10531491041183472, 0.26949483156204224, 0.3588126301765442, 0.19936376810073853, 0.5471915602684021, 0.006160438060760498, 0.951554536819458, 0.07526588439941406]\n",
            "      After:  [1.7269139289855957, 1.1142067909240723, 0.5775636434555054, 1.577864408493042, 0.3694651126861572, 1.4363677501678467, 0.32853835821151733, 1.8311855792999268, 0.11531474441289902, 1.2694448232650757, 0.36197489500045776, 1.1993587017059326, 0.5481915473937988, 1.006160020828247, 0.9518707394599915, 1.075265884399414]\n",
            "    Token 2:\n",
            "      Before: [0.8860136866569519, 0.5832095742225647, 0.3376477360725403, 0.8089749813079834, 0.5779253840446472, 0.9039816856384277, 0.5546598434448242, 0.34231340885162354, 0.634341835975647, 0.36441028118133545, 0.710428774356842, 0.9464110732078552, 0.7890297770500183, 0.281413733959198, 0.788632333278656, 0.5894631147384644]\n",
            "      After:  [1.7953110933303833, 0.16706272959709167, 0.9287748336791992, 1.615553379058838, 0.7765946984291077, 1.8840482234954834, 0.6178632378578186, 1.3403141498565674, 0.6543405055999756, 1.3642103672027588, 0.7167533040046692, 1.9463911056518555, 0.7910297513008118, 1.2814117670059204, 0.7892647981643677, 1.5894629955291748]\n",
            "    Token 3:\n",
            "      Before: [0.7539175152778625, 0.19524747133255005, 0.005045771598815918, 0.30681973695755005, 0.11648857593536377, 0.9102694392204285, 0.6440156698226929, 0.7071067690849304, 0.6581305861473083, 0.4913020133972168, 0.8913041353225708, 0.1447432041168213, 0.5314818620681763, 0.1587299108505249, 0.6541759967803955, 0.32780885696411133]\n",
            "      After:  [0.8950375318527222, -0.7947450280189514, 0.8176946640014648, 0.8895733952522278, 0.41200879216194153, 1.8656059503555298, 0.7387417554855347, 1.7026101350784302, 0.6881260871887207, 1.4908521175384521, 0.900790810585022, 1.144698143005371, 0.5344818830490112, 1.1587255001068115, 0.6551246643066406, 1.3278083801269531]\n",
            "    Token 4:\n",
            "      Before: [0.6532081365585327, 0.3958292603492737, 0.9146959185600281, 0.20364904403686523, 0.20180100202560425, 0.20178300142288208, 0.9497213959693909, 0.6666255593299866, 0.9811253547668457, 0.08736187219619751, 0.00406193733215332, 0.10881811380386353, 0.16365545988082886, 0.7025200724601746, 0.6790379285812378, 0.9154621958732605]\n",
            "      After:  [-0.10359436273574829, -0.25781434774398804, 1.868276596069336, 0.504786491394043, 0.5912193655967712, 1.1228439807891846, 1.0758755207061768, 1.658636212348938, 1.021114706993103, 1.086561918258667, 0.016710711643099785, 1.1087381839752197, 0.16765545308589935, 1.7025120258331299, 0.6803028583526611, 1.9154614210128784]\n",
            "    Token 5:\n",
            "      Before: [0.24178731441497803, 0.1591441035270691, 0.7652890682220459, 0.2978977560997009, 0.8034619092941284, 0.38134968280792236, 0.786022961139679, 0.11151599884033203, 0.2476751208305359, 0.652438223361969, 0.6057037711143494, 0.3725206255912781, 0.7980347275733948, 0.8399046063423157, 0.13741332292556763, 0.2330659031867981]\n",
            "      After:  [-0.7171369791030884, 0.44280630350112915, 1.7652356624603271, 0.28755542635917664, 1.2828874588012695, 1.2589322328567505, 0.9434788227081299, 1.0990419387817383, 0.29765430092811584, 1.6511884927749634, 0.6215144991874695, 1.372395634651184, 0.8030347228050232, 1.8398921489715576, 0.13899445533752441, 1.2330646514892578]\n",
            "    Token 6:\n",
            "      Before: [0.9578309655189514, 0.3312837481498718, 0.3227418065071106, 0.016202688217163086, 0.21366488933563232, 0.6249018311500549, 0.43400341272354126, 0.13705700635910034, 0.5117283463478088, 0.15845924615859985, 0.07580167055130005, 0.2246686816215515, 0.06239396333694458, 0.1816309690475464, 0.9998044371604919, 0.5944374799728394]\n",
            "      After:  [0.6784154772758484, 1.2914540767669678, 1.2698900699615479, -0.3045937120914459, 0.7783073782920837, 1.450237512588501, 0.622603714466095, 1.1191109418869019, 0.5716923475265503, 1.1566598415374756, 0.09477420151233673, 1.2244887351989746, 0.06839393079280853, 1.1816129684448242, 1.001701831817627, 1.594435691833496]\n",
            "\n",
            "Window 1 (from document 1):\n",
            "  Original tokens: [2, 4, 5, 6, 9, 10, 11, 12, 13, 14]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> This is a much longer sentence that will be'\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Positional encoding effect:\n",
            "    Token 0:\n",
            "      Before: [0.9544757604598999, 0.6098752021789551, 0.5643200278282166, 0.05937260389328003, 0.7098942399024963, 0.4249897003173828, 0.27093786001205444, 0.9294732809066772, 0.6114743947982788, 0.2233617901802063, 0.2469305396080017, 0.4761221408843994, 0.779180645942688, 0.3722330927848816, 0.21471256017684937, 0.32877856492996216]\n",
            "      After:  [0.9544757604598999, 1.609875202178955, 0.5643200278282166, 1.0593726634979248, 0.7098942399024963, 1.4249897003173828, 0.27093786001205444, 1.9294732809066772, 0.6114743947982788, 1.2233617305755615, 0.2469305396080017, 1.4761221408843994, 0.779180645942688, 1.3722331523895264, 0.21471256017684937, 1.3287785053253174]\n",
            "    Token 1:\n",
            "      Before: [0.12646257877349854, 0.6783162355422974, 0.8870201110839844, 0.02927982807159424, 0.6161253452301025, 0.7582958936691284, 0.5906646847724915, 0.3219376802444458, 0.7609710693359375, 0.7627565860748291, 0.6869636178016663, 0.41213929653167725, 0.36759936809539795, 0.5534904599189758, 0.4116729497909546, 0.35099947452545166]\n",
            "      After:  [0.9679335355758667, 1.218618631362915, 1.1980037689208984, 0.9796951413154602, 0.7159587740898132, 1.7533000707626343, 0.6222822070121765, 1.3214377164840698, 0.77097088098526, 1.7627065181732178, 0.6901258826255798, 1.4121342897415161, 0.3685993552207947, 1.5534899234771729, 0.4119891822338104, 1.350999355316162]\n",
            "    Token 2:\n",
            "      Before: [0.819603443145752, 0.9296997785568237, 0.45050132274627686, 0.3880515694618225, 0.5072961449623108, 0.4701458811759949, 0.6202056407928467, 0.640116810798645, 0.045871615409851074, 0.31548112630844116, 0.9210647344589233, 0.6947774887084961, 0.47513121366500854, 0.1985471248626709, 0.19409745931625366, 0.052116572856903076]\n",
            "      After:  [1.7289009094238281, 0.5135529041290283, 1.041628360748291, 1.1946299076080322, 0.7059654593467712, 1.4502124786376953, 0.6834090352058411, 1.6381175518035889, 0.06587028503417969, 1.3152811527252197, 0.9273892641067505, 1.6947574615478516, 0.4771312177181244, 1.198545217514038, 0.19472990930080414, 1.0521163940429688]\n",
            "    Token 3:\n",
            "      Before: [0.33701878786087036, 0.6688520908355713, 0.8188108205795288, 0.7308486700057983, 0.05802798271179199, 0.1993187665939331, 0.4210916757583618, 0.9836747646331787, 0.5723287463188171, 0.3705146312713623, 0.7068576216697693, 0.3095592260360718, 0.17637217044830322, 0.8649436235427856, 0.2726491093635559, 0.39976662397384644]\n",
            "      After:  [0.47813880443573, -0.3211404085159302, 1.6314597129821777, 1.313602328300476, 0.35354819893836975, 1.1546552181243896, 0.5158177614212036, 1.9791781902313232, 0.6023242473602295, 1.3700647354125977, 0.7163442969322205, 1.3095142841339111, 0.1793721616268158, 1.8649392127990723, 0.2735978066921234, 1.399766206741333]\n",
            "    Token 4:\n",
            "      Before: [0.0025978684425354004, 0.834635317325592, 0.8788173198699951, 0.6822240948677063, 0.15136289596557617, 0.006530046463012695, 0.09391051530838013, 0.8728501200675964, 0.7400528788566589, 0.920752227306366, 0.7619349360466003, 0.6265460848808289, 0.495103657245636, 0.11974698305130005, 0.07161390781402588, 0.032325685024261475]\n",
            "      After:  [-0.7542046308517456, 0.18099170923233032, 1.8323980569839478, 0.9833616018295288, 0.5407812595367432, 0.9275910258293152, 0.22006459534168243, 1.8648607730865479, 0.7800422310829163, 1.919952392578125, 0.7745836973190308, 1.6264660358428955, 0.4991036355495453, 1.119739055633545, 0.07287881523370743, 1.0323249101638794]\n",
            "    Token 5:\n",
            "      Before: [0.7046809792518616, 0.25451600551605225, 0.39937371015548706, 0.21224737167358398, 0.40888822078704834, 0.14808255434036255, 0.1732921600341797, 0.6658554077148438, 0.35140180587768555, 0.8086715936660767, 0.33959561586380005, 0.13321638107299805, 0.41178053617477417, 0.2576262950897217, 0.3470292091369629, 0.02400219440460205]\n",
            "      After:  [-0.25424331426620483, 0.5381782054901123, 1.3993202447891235, 0.2019050270318985, 0.8883137702941895, 1.025665044784546, 0.330748051404953, 1.65338134765625, 0.4013809859752655, 1.8074219226837158, 0.35540634393692017, 1.1330914497375488, 0.4167805016040802, 1.2576137781143188, 0.3486103415489197, 1.024000883102417]\n",
            "    Token 6:\n",
            "      Before: [0.7797454595565796, 0.15189772844314575, 0.7513088583946228, 0.7268921136856079, 0.8572163581848145, 0.1164739727973938, 0.8595983982086182, 0.2636241912841797, 0.6855345964431763, 0.9695573449134827, 0.4294840693473816, 0.4961332678794861, 0.38488471508026123, 0.08250772953033447, 0.7399514317512512, 0.003641068935394287]\n",
            "      After:  [0.5003299713134766, 1.1120679378509521, 1.6984570026397705, 0.4060957133769989, 1.421858787536621, 0.9418095946311951, 1.0481986999511719, 1.245678186416626, 0.7454985976219177, 1.9677579402923584, 0.4484565854072571, 1.4959533214569092, 0.390884667634964, 1.0824897289276123, 0.7418488264083862, 1.0036392211914062]\n",
            "    Token 7:\n",
            "      Before: [0.8103999495506287, 0.8741125464439392, 0.9728531837463379, 0.3820602297782898, 0.08917903900146484, 0.6124151349067688, 0.7762136459350586, 0.0023456215858459473, 0.38650816679000854, 0.20027226209640503, 0.4562681317329407, 0.25389325618743896, 0.2956162095069885, 0.3412705659866333, 0.024847984313964844, 0.9102537631988525]\n",
            "      After:  [1.4673864841461182, 1.6280148029327393, 1.7732747793197632, -0.21737724542617798, 0.7333967089653015, 1.3772573471069336, 0.995769739151001, 0.9779455065727234, 0.4564509987831116, 1.1978232860565186, 0.4784022569656372, 1.253648281097412, 0.302616149187088, 1.3412461280822754, 0.027061577886343002, 1.9102513790130615]\n",
            "    Token 8:\n",
            "      Before: [0.9191656112670898, 0.4215654730796814, 0.44305896759033203, 0.2959400415420532, 0.048468589782714844, 0.013427793979644775, 0.685829222202301, 0.2254769206047058, 0.1785615086555481, 0.4609884023666382, 0.3334944248199463, 0.3382396101951599, 0.5160655975341797, 0.39394378662109375, 0.3278437852859497, 0.2605970501899719]\n",
            "      After:  [1.9085237979888916, 0.2760654389858246, 1.0173767805099487, -0.5226923823356628, 0.7658246755599976, 0.7101345062255859, 0.9361215829849243, 1.1936471462249756, 0.258476197719574, 1.4577901363372803, 0.35878995060920715, 1.3379197120666504, 0.5240654945373535, 1.3939118385314941, 0.330373615026474, 1.2605938911437988]\n",
            "    Token 9:\n",
            "      Before: [0.09308630228042603, 0.9192535877227783, 0.2999064326286316, 0.6324897408485413, 0.3265170454978943, 0.5406306385993958, 0.9661502242088318, 0.7303613424301147, 0.06670016050338745, 0.6984513998031616, 0.9746214151382446, 0.6315416693687439, 0.8352123498916626, 0.9929437637329102, 0.42338550090789795, 0.6037772297859192]\n",
            "      After:  [0.5052047967910767, 0.008123338222503662, 0.5911656618118286, -0.32415443658828735, 1.1098439693450928, 1.1622406244277954, 1.2469285726547241, 1.690134048461914, 0.15657871961593628, 1.694404125213623, 1.0030781030654907, 1.6311366558074951, 0.8442122340202332, 1.992903232574463, 0.4262315332889557, 1.6037731170654297]\n",
            "\n",
            "Window 2 (from document 1):\n",
            "  Original tokens: [2, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> will be split into multiple windows. Observe the term'\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Positional encoding effect:\n",
            "    Token 0:\n",
            "      Before: [0.15248245000839233, 0.3969614505767822, 0.8702918887138367, 0.7563229203224182, 0.18360549211502075, 0.09905749559402466, 0.1583181619644165, 0.006561160087585449, 0.11418050527572632, 0.3763512969017029, 0.8374385833740234, 0.5836911201477051, 0.11969727277755737, 0.09888803958892822, 0.748737633228302, 0.128079354763031]\n",
            "      After:  [0.15248245000839233, 1.3969614505767822, 0.8702918887138367, 1.7563228607177734, 0.18360549211502075, 1.0990574359893799, 0.1583181619644165, 1.0065611600875854, 0.11418050527572632, 1.3763513565063477, 0.8374385833740234, 1.583691120147705, 0.11969727277755737, 1.0988880395889282, 0.748737633228302, 1.1280794143676758]\n",
            "    Token 1:\n",
            "      Before: [0.43843626976013184, 0.739853024482727, 0.268593966960907, 0.4454800486564636, 0.4564777612686157, 0.3817083239555359, 0.2464839220046997, 0.05428081750869751, 0.09582149982452393, 0.23226916790008545, 0.9829188585281372, 0.258492648601532, 0.16423600912094116, 0.6211971044540405, 0.637805163860321, 0.7739548683166504]\n",
            "      After:  [1.2799072265625, 1.2801554203033447, 0.5795775651931763, 1.3958953619003296, 0.5563111901283264, 1.3767125606536865, 0.27810144424438477, 1.0537807941436768, 0.10582133382558823, 1.2322192192077637, 0.9860811233520508, 1.2584877014160156, 0.16523601114749908, 1.6211966276168823, 0.6381213665008545, 1.7739548683166504]\n",
            "    Token 2:\n",
            "      Before: [0.8800601959228516, 0.778437077999115, 0.004249513149261475, 0.5443443059921265, 0.8028765320777893, 0.45378726720809937, 0.20536041259765625, 0.9766699075698853, 0.3129860758781433, 0.21532773971557617, 0.049222469329833984, 0.5223341584205627, 0.7215665578842163, 0.610681414604187, 0.5988748669624329, 0.12080627679824829]\n",
            "      After:  [1.7893576622009277, 0.36229023337364197, 0.5953766107559204, 1.350922703742981, 1.0015459060668945, 1.4338538646697998, 0.26856380701065063, 1.974670648574829, 0.3329847455024719, 1.21512770652771, 0.05554698407649994, 1.522314190864563, 0.7235665321350098, 1.6106793880462646, 0.5995073318481445, 1.120806097984314]\n",
            "    Token 3:\n",
            "      Before: [0.03305637836456299, 0.5088046789169312, 0.9559170603752136, 0.7884606719017029, 0.2088828682899475, 0.43509572744369507, 0.1314082145690918, 0.2587882876396179, 0.5905491709709167, 0.7722692489624023, 0.9141846299171448, 0.04094696044921875, 0.8343076109886169, 0.14735394716262817, 0.6872336268424988, 0.9231226444244385]\n",
            "      After:  [0.1741763800382614, -0.4811878204345703, 1.7685658931732178, 1.3712143898010254, 0.5044031143188477, 1.3904322385787964, 0.2261343002319336, 1.2542916536331177, 0.6205446720123291, 1.7718193531036377, 0.923671305179596, 1.0409018993377686, 0.8373076319694519, 1.14734947681427, 0.6881822943687439, 1.9231221675872803]\n",
            "    Token 4:\n",
            "      Before: [0.5070211887359619, 0.9549044966697693, 0.07397425174713135, 0.30902040004730225, 0.7916264533996582, 0.3910660743713379, 0.397649884223938, 0.2916041612625122, 0.8446530699729919, 0.7452515959739685, 0.6602250337600708, 0.21901816129684448, 0.09412521123886108, 0.5540803074836731, 0.6481394171714783, 0.26914405822753906]\n",
            "      After:  [-0.2497813105583191, 0.30126088857650757, 1.027554988861084, 0.61015784740448, 1.1810448169708252, 1.3121271133422852, 0.5238039493560791, 1.2836148738861084, 0.8846424221992493, 1.7444517612457275, 0.6728737950325012, 1.2189381122589111, 0.09812520444393158, 1.554072380065918, 0.6494043469429016, 1.2691433429718018]\n",
            "    Token 5:\n",
            "      Before: [0.3601011633872986, 0.8376838564872742, 0.5398298501968384, 0.5225591659545898, 0.3769497275352478, 0.04720515012741089, 0.02987128496170044, 0.26099246740341187, 0.24583929777145386, 0.6557767987251282, 0.35444462299346924, 0.30438894033432007, 0.9767149090766907, 0.674161434173584, 0.856451153755188, 0.25794363021850586]\n",
            "      After:  [-0.5988231301307678, 1.1213459968566895, 1.53977632522583, 0.5122168064117432, 0.8563752770423889, 0.924787700176239, 0.18732717633247375, 1.248518466949463, 0.2958184778690338, 1.6545270681381226, 0.37025535106658936, 1.304263949394226, 0.9817149043083191, 1.6741489171981812, 0.8580322861671448, 1.2579424381256104]\n",
            "    Token 6:\n",
            "      Before: [0.29576659202575684, 0.6837702393531799, 0.16686242818832397, 0.17314797639846802, 0.4758501648902893, 0.31711965799331665, 0.1251710057258606, 0.7965794801712036, 0.9020814299583435, 0.5811116695404053, 0.41294336318969727, 0.036863505840301514, 0.31788063049316406, 0.627292811870575, 0.7357654571533203, 0.43679124116897583]\n",
            "      After:  [0.01635110378265381, 1.6439404487609863, 1.1140105724334717, -0.147648423910141, 1.0404926538467407, 1.1424553394317627, 0.3137713074684143, 1.7786333560943604, 0.962045431137085, 1.5793122053146362, 0.43191587924957275, 1.0366835594177246, 0.3238805830478668, 1.627274751663208, 0.7376628518104553, 1.4367895126342773]\n",
            "    Token 7:\n",
            "      Before: [0.302323579788208, 0.7786130309104919, 0.10180014371871948, 0.816008985042572, 0.306022584438324, 0.5076526999473572, 0.4011920690536499, 0.5606194734573364, 0.34890079498291016, 0.8635634779930115, 0.4870014190673828, 0.8902997374534607, 0.9807402491569519, 0.2564045190811157, 0.1352454423904419, 0.9011510014533997]\n",
            "      After:  [0.9593101739883423, 1.532515287399292, 0.9022217392921448, 0.21657150983810425, 0.9502402544021606, 1.272494912147522, 0.6207481622695923, 1.5362193584442139, 0.4188436269760132, 1.861114501953125, 0.5091355443000793, 1.890054702758789, 0.9877402186393738, 1.2563800811767578, 0.13745903968811035, 1.9011485576629639]\n",
            "    Token 8:\n",
            "      Before: [0.891806960105896, 0.11822634935379028, 0.46134835481643677, 0.006936848163604736, 0.09070044755935669, 0.5965712666511536, 0.6330173015594482, 0.6059905290603638, 0.36391764879226685, 0.9612888693809509, 0.5714889764785767, 0.20495760440826416, 0.47169309854507446, 0.620072603225708, 0.675096333026886, 0.14645957946777344]\n",
            "      After:  [1.8811652660369873, -0.02727368474006653, 1.0356662273406982, -0.8116955757141113, 0.8080565333366394, 1.2932779788970947, 0.8833096623420715, 1.5741608142852783, 0.4438323378562927, 1.9580905437469482, 0.5967844724655151, 1.2046376466751099, 0.4796930253505707, 1.6200406551361084, 0.6776261329650879, 1.1464563608169556]\n",
            "    Token 9:\n",
            "      Before: [0.6873947978019714, 0.2445591688156128, 0.0845298171043396, 0.2268962860107422, 0.9822046756744385, 0.9274289011955261, 0.947742223739624, 0.7935056090354919, 0.8777247667312622, 0.4330751299858093, 0.22488605976104736, 0.7498282790184021, 0.24090862274169922, 0.16256707906723022, 0.3403329849243164, 0.6049296259880066]\n",
            "      After:  [1.099513292312622, -0.6665710806846619, 0.3757890462875366, -0.7297478914260864, 1.7655315399169922, 1.5490388870239258, 1.2285206317901611, 1.7532782554626465, 0.967603325843811, 1.429027795791626, 0.25334271788597107, 1.7494232654571533, 0.24990850687026978, 1.1625266075134277, 0.34317901730537415, 1.6049256324768066]\n",
            "\n",
            "Window 3 (from document 1):\n",
            "  Original tokens: [2, 20, 21, 22, 3, 0, 0, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "  Decoded: '<SOS> the term overlapping? <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Positional encoding effect:\n",
            "    Token 0:\n",
            "      Before: [0.7573983073234558, 0.30579549074172974, 0.20571684837341309, 0.5674465298652649, 0.20528340339660645, 0.17446929216384888, 0.760625958442688, 0.4160076975822449, 0.9568924903869629, 0.9863913059234619, 0.6495527625083923, 0.6720788478851318, 0.6151418685913086, 0.5078304409980774, 0.46363377571105957, 0.5068720579147339]\n",
            "      After:  [0.7573983073234558, 1.305795431137085, 0.20571684837341309, 1.5674464702606201, 0.20528340339660645, 1.174469232559204, 0.760625958442688, 1.4160077571868896, 0.9568924903869629, 1.986391305923462, 0.6495527625083923, 1.6720788478851318, 0.6151418685913086, 1.5078303813934326, 0.46363377571105957, 1.5068720579147339]\n",
            "    Token 1:\n",
            "      Before: [0.686712384223938, 0.964885413646698, 0.3704204559326172, 0.28864210844039917, 0.3789175748825073, 0.2584378719329834, 0.5850193500518799, 0.8732241988182068, 0.8909887075424194, 0.7295627593994141, 0.13203424215316772, 0.23164761066436768, 0.3901442885398865, 0.4078379273414612, 0.5411238670349121, 0.041014254093170166]\n",
            "      After:  [1.5281833410263062, 1.505187749862671, 0.6814040541648865, 1.2390574216842651, 0.478751003742218, 1.2534420490264893, 0.6166368722915649, 1.8727242946624756, 0.9009885191917419, 1.7295126914978027, 0.1351965218782425, 1.2316426038742065, 0.3911442756652832, 1.4078373908996582, 0.5414400696754456, 1.0410141944885254]\n",
            "    Token 2:\n",
            "      Before: [0.6556223630905151, 0.1185639500617981, 0.1836276650428772, 0.08430874347686768, 0.9356598258018494, 0.026530086994171143, 0.8771833777427673, 0.4831915497779846, 0.44185060262680054, 0.8127392530441284, 0.4537861943244934, 0.8135770559310913, 0.8615074753761292, 0.0658949613571167, 0.6923919916152954, 0.5943894982337952]\n",
            "      After:  [1.5649197101593018, -0.2975828945636749, 0.7747547626495361, 0.8908871412277222, 1.1343291997909546, 1.0065966844558716, 0.9403867721557617, 1.4811922311782837, 0.46184927225112915, 1.8125393390655518, 0.46011069416999817, 1.8135571479797363, 0.8635074496269226, 1.0658929347991943, 0.6930244565010071, 1.5943893194198608]\n",
            "    Token 3:\n",
            "      Before: [0.6075058579444885, 0.5729957222938538, 0.6367654800415039, 0.25946658849716187, 0.43602943420410156, 0.975059986114502, 0.8359247446060181, 0.48121577501296997, 0.029734551906585693, 0.5219138860702515, 0.15951323509216309, 0.906595766544342, 0.19645631313323975, 0.4638991951942444, 0.3890286684036255, 0.5889769196510315]\n",
            "      After:  [0.7486258745193481, -0.4169967770576477, 1.4494143724441528, 0.8422202467918396, 0.7315496206283569, 1.930396556854248, 0.9306508302688599, 1.4767191410064697, 0.05973005294799805, 1.5214638710021973, 0.16899992525577545, 1.9065507650375366, 0.19945630431175232, 1.4638947248458862, 0.389977365732193, 1.588976502418518]\n",
            "    Token 4:\n",
            "      Before: [0.9705138206481934, 0.5475096106529236, 0.7895820140838623, 0.8881108164787292, 0.9036555886268616, 0.3273242712020874, 0.3881716728210449, 0.7409688830375671, 0.36356616020202637, 0.7341319918632507, 0.3907661437988281, 0.16087383031845093, 0.7035216689109802, 0.5766590237617493, 0.7229241728782654, 0.9967430233955383]\n",
            "      After:  [0.21371132135391235, -0.10613399744033813, 1.743162751197815, 1.1892483234405518, 1.2930738925933838, 1.2483851909637451, 0.514325737953186, 1.7329795360565186, 0.4035554826259613, 1.7333321571350098, 0.40341490507125854, 1.1607937812805176, 0.7075216770172119, 1.5766510963439941, 0.7241891026496887, 1.9967422485351562]\n",
            "\n",
            "Window 4 (from document 2):\n",
            "  Original tokens: [2, 23, 11, 24, 25, 26, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> Another sentence of medium length. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Positional encoding effect:\n",
            "    Token 0:\n",
            "      Before: [0.6020277142524719, 0.03156214952468872, 0.9365567564964294, 0.8136954307556152, 0.010527074337005615, 0.261183500289917, 0.6630775928497314, 0.39727020263671875, 0.44551175832748413, 0.2742421627044678, 0.9016097784042358, 0.2205008864402771, 0.9146384000778198, 0.5322611331939697, 0.6005108952522278, 0.8900659084320068]\n",
            "      After:  [0.6020277142524719, 1.031562089920044, 0.9365567564964294, 1.8136954307556152, 0.010527074337005615, 1.261183500289917, 0.6630775928497314, 1.3972702026367188, 0.44551175832748413, 1.2742421627044678, 0.9016097784042358, 1.2205009460449219, 0.9146384000778198, 1.5322611331939697, 0.6005108952522278, 1.8900659084320068]\n",
            "    Token 1:\n",
            "      Before: [0.41761720180511475, 0.21532833576202393, 0.41913288831710815, 0.9055266976356506, 0.12900632619857788, 0.6134902238845825, 0.008604288101196289, 0.7621510624885559, 0.6847338676452637, 0.5211961269378662, 0.7145965695381165, 0.5005623698234558, 0.7766764163970947, 0.10418975353240967, 0.4265737533569336, 0.7218073010444641]\n",
            "      After:  [1.259088158607483, 0.7556306719779968, 0.7301164865493774, 1.8559420108795166, 0.22883975505828857, 1.6084944009780884, 0.040221795439720154, 1.7616510391235352, 0.6947336792945862, 1.5211460590362549, 0.71775883436203, 1.5005574226379395, 0.7776764035224915, 1.1041892766952515, 0.42688998579978943, 1.7218072414398193]\n",
            "    Token 2:\n",
            "      Before: [0.9979084134101868, 0.75469571352005, 0.13641279935836792, 0.8845484256744385, 0.3885008692741394, 0.3932427763938904, 0.04554516077041626, 0.42129284143447876, 0.8536633849143982, 0.5697224140167236, 0.20877301692962646, 0.6539060473442078, 0.3396778106689453, 0.9564970135688782, 0.06602269411087036, 0.34206223487854004]\n",
            "      After:  [1.9072058200836182, 0.338548868894577, 0.7275398969650269, 1.691126823425293, 0.5871701836585999, 1.3733093738555908, 0.10874856263399124, 1.4192935228347778, 0.8736620545387268, 1.5695223808288574, 0.21509753167629242, 1.653886079788208, 0.34167781472206116, 1.9564950466156006, 0.06665515154600143, 1.342061996459961]\n",
            "    Token 3:\n",
            "      Before: [0.01721322536468506, 0.3030849099159241, 0.657623827457428, 0.981307327747345, 0.5839731693267822, 0.9901792407035828, 0.5978260636329651, 0.7887679934501648, 0.9008311033248901, 0.9179616570472717, 0.22013813257217407, 0.9596949815750122, 0.802882730960846, 0.26621049642562866, 0.2613983154296875, 0.08062690496444702]\n",
            "      After:  [0.15833322703838348, -0.6869075894355774, 1.4702727794647217, 1.564060926437378, 0.8794933557510376, 1.945515751838684, 0.6925521492958069, 1.7842713594436646, 0.9308266043663025, 1.9175117015838623, 0.22962482273578644, 1.9596500396728516, 0.8058827519416809, 1.2662060260772705, 0.262347012758255, 1.0806264877319336]\n",
            "    Token 4:\n",
            "      Before: [0.6255686283111572, 0.09472537040710449, 0.7112123370170593, 0.6578988432884216, 0.06559890508651733, 0.6362504363059998, 0.45933473110198975, 0.7284088730812073, 0.7868947982788086, 0.0029274821281433105, 0.958549439907074, 0.9193210005760193, 0.6989418268203735, 0.04301947355270386, 0.3213896155357361, 0.3550955653190613]\n",
            "      After:  [-0.13123387098312378, -0.5589182376861572, 1.6647930145263672, 0.9590363502502441, 0.4550172686576843, 1.5573114156723022, 0.5854887962341309, 1.7204195261001587, 0.8268841505050659, 1.0021276473999023, 0.9711982011795044, 1.919240951538086, 0.7029418349266052, 1.0430114269256592, 0.32265451550483704, 1.3550947904586792]\n",
            "    Token 5:\n",
            "      Before: [0.37150102853775024, 0.7819615602493286, 0.6817852854728699, 0.8960895538330078, 0.31273841857910156, 0.6682698726654053, 0.677897572517395, 0.08370459079742432, 0.014990091323852539, 0.24055546522140503, 0.8422738313674927, 0.029270172119140625, 0.06478309631347656, 0.7801002860069275, 0.7697644829750061, 0.9111963510513306]\n",
            "      After:  [-0.5874232649803162, 1.0656237602233887, 1.6817318201065063, 0.8857471942901611, 0.7921639680862427, 1.5458524227142334, 0.8353534936904907, 1.0712306499481201, 0.0649692565202713, 1.2393057346343994, 0.8580845594406128, 1.0291452407836914, 0.06978307664394379, 1.7800877094268799, 0.7713456153869629, 1.9111950397491455]\n",
            "    Token 6:\n",
            "      Before: [0.12253063917160034, 0.1340501308441162, 0.756493330001831, 0.9348151087760925, 0.7991694211959839, 0.5783260464668274, 0.6647873520851135, 0.9745633602142334, 0.17739784717559814, 0.27299410104751587, 0.8497334718704224, 0.15788018703460693, 0.22429370880126953, 0.864995539188385, 0.6577610373497009, 0.6615350246429443]\n",
            "      After:  [-0.15688484907150269, 1.0942203998565674, 1.7036415338516235, 0.6140186786651611, 1.36381196975708, 1.4036617279052734, 0.8533876538276672, 1.9566173553466797, 0.2373618483543396, 1.2711946964263916, 0.8687059879302979, 1.1577001810073853, 0.23029367625713348, 1.8649775981903076, 0.6596584320068359, 1.661533236503601]\n",
            "\n",
            "--- End of Test ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Masked Multihead Attention Mechanism\n",
        "\n",
        "As you have implemented the Attention Mechanism in Part I, here you will have to use its ready implementation from Pytorch.\n",
        "\n",
        "Masked Attention mechanism allows the transformer model to focus on relevant parts of the input sequence while preventing information leakage from future tokens during sequential processing (i.e. we use the term Masked).\n",
        "\n",
        "Please, visit [Link-pytorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for a detailed description of the MultiheadAttention function in PyTorch.\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   Multi-head Attention, deep dive [Link-towardsdatascience](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
        "* Attention Is All You Need (original Transformer paper) [Link-ArXiv](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* A visual explanation of the attention mechanism [Link-youtube](https://www.youtube.com/watch?v=bCz4OMemCcA&t=1208s&ab_channel=UmarJamil)"
      ],
      "metadata": {
        "id": "i_d5KqeSAdiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        #######################Code Here###############################\n",
        "\n",
        "        # Initialize the MultiheadAttention layer using its PyTorch implementation\n",
        "\n",
        "        # Initialize the parameters\n",
        "\n",
        "        ###############################################################\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn._reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "\n",
        "        #######################Code Here###############################\n",
        "        # Create a triangular mask tensor in PyTorch where the lower triangle (including the diagonal) is\n",
        "        # filled with ones and the upper triangle is filled with zeros. The tensor should be of size (sz, sz),\n",
        "        # where sz is a given integer.\n",
        "        ##############################################################\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "\n",
        "        # The masked positions are filled with float('-inf'). Unmasked1_nlp positions are filled with float(0.0). See: https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        seq_len = x.size(1)\n",
        "        attn_mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        output, _ = self.multihead_attn(x, x, x, attn_mask=attn_mask)\n",
        "        return output, attn_mask"
      ],
      "metadata": {
        "id": "oDqt8XggAlFe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Multihead Attention"
      ],
      "metadata": {
        "id": "MAMRpHUKwGKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_expanded_transformer_components():\n",
        "    print(\"\\n--- Testing Expanded Transformer Components ---\\n\")\n",
        "\n",
        "    # Set a fixed seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Sample texts\n",
        "    texts = [\n",
        "        \"This is a short sentence.\",\n",
        "        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n",
        "        \"Another sentence of medium length.\"\n",
        "    ]\n",
        "\n",
        "    # Initialize tokenizer and fit it to the texts\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    tokenizer.fit(texts)\n",
        "\n",
        "    # Create dataset\n",
        "    max_length = 10\n",
        "    overlap = 2\n",
        "    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n",
        "\n",
        "    # Hyperparameters\n",
        "    d_model = 16  # Small dimension for demonstration\n",
        "    nhead = 2\n",
        "\n",
        "    # Initialize components\n",
        "    pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "    masked_self_attn = MaskedAttention(d_model, nhead)\n",
        "\n",
        "    print(f\"Dataset configuration:\")\n",
        "    print(f\"  Max length: {max_length}\")\n",
        "    print(f\"  Overlap: {overlap}\")\n",
        "    print(f\"  Total windows: {len(dataset)}\")\n",
        "    print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
        "    print(f\"  Embedding dimension: {d_model}\")\n",
        "    print(f\"  Number of attention heads: {nhead}\\n\")\n",
        "\n",
        "    # Process each window\n",
        "    for i in range(len(dataset)):\n",
        "        tokens, attention_mask, doc_idx = dataset[i]\n",
        "\n",
        "        print(f\"Window {i} (from document {doc_idx}):\")\n",
        "        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n",
        "        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n",
        "        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n",
        "\n",
        "        # Convert tokens to \"embeddings\" (just for demonstration)\n",
        "        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n",
        "        print(f\"  Shape of pseudo embeddings: {pseudo_embeddings.shape}\")\n",
        "\n",
        "        # Apply positional encoding\n",
        "        pos_encoded = pos_encoder(pseudo_embeddings)\n",
        "        print(f\"  Shape after positional encoding: {pos_encoded.shape}\")\n",
        "\n",
        "        # Apply masked self-attention\n",
        "        attn_output, attn_mask = masked_self_attn(pos_encoded)\n",
        "        print(f\"  Shape after masked self-attention: {attn_output.shape}\")\n",
        "\n",
        "        # Display the effect of positional encoding and attention for all tokens\n",
        "        print(f\"  Transformer effect on tokens:\")\n",
        "        for j in range(tokens.size(1)):\n",
        "            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n",
        "                print(f\"    Token {j}:\")\n",
        "                print(f\"      Initial:   {pseudo_embeddings[0, j, :5].tolist()}\")\n",
        "                print(f\"      Positional:{pos_encoded[0, j, :5].tolist()}\")\n",
        "                print(f\"      Attention Mask: {attn_mask[j, :5].tolist()}\")  # Show first 5 values of attention mask\n",
        "                print(f\"      Attention: {attn_output[0, j, :5].tolist()}\")\n",
        "        print()\n",
        "\n",
        "    print(\"--- End of Expanded Test ---\")\n",
        "\n",
        "# Run the expanded test\n",
        "test_expanded_transformer_components()"
      ],
      "metadata": {
        "id": "e0UqHQVswJzX",
        "outputId": "3a7613b0-19d4-406f-a8ac-41b2e76f0594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Expanded Transformer Components ---\n",
            "\n",
            "Dataset configuration:\n",
            "  Max length: 10\n",
            "  Overlap: 2\n",
            "  Total windows: 5\n",
            "  Vocabulary size: 27\n",
            "  Embedding dimension: 16\n",
            "  Number of attention heads: 2\n",
            "\n",
            "Window 0 (from document 0):\n",
            "  Original tokens: [2, 4, 5, 6, 7, 8, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> This is a short sentence. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.884685218334198, 0.2102099061012268, 0.5239563584327698, 0.11974334716796875, 0.26555144786834717]\n",
            "      Positional:[0.884685218334198, 1.210209846496582, 0.5239563584327698, 1.1197433471679688, 0.26555144786834717]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.04382609575986862, -0.20037822425365448, -0.10608989000320435, -0.46559810638427734, -1.238613247871399]\n",
            "    Token 1:\n",
            "      Initial:   [0.7821199893951416, 0.3691885471343994, 0.8093817830085754, 0.10163873434066772, 0.32407933473587036]\n",
            "      Positional:[1.6235909461975098, 0.9094908833503723, 1.1203653812408447, 1.0520540475845337, 0.42391276359558105]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [-0.022696254774928093, -0.16313883662223816, -0.10051843523979187, -0.37186479568481445, -1.2318873405456543]\n",
            "    Token 2:\n",
            "      Initial:   [0.32780128717422485, 0.8388155102729797, 0.7823862433433533, 0.254031777381897, 0.005369961261749268]\n",
            "      Positional:[1.2370986938476562, 0.4226686656475067, 1.3735133409500122, 1.0606101751327515, 0.2040392905473709]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [-0.1380462646484375, -0.2636273205280304, -0.13520675897598267, -0.41493186354637146, -1.2141354084014893]\n",
            "    Token 3:\n",
            "      Initial:   [0.170424222946167, 0.05281209945678711, 0.01924741268157959, 0.8139157295227051, 0.675154983997345]\n",
            "      Positional:[0.3115442395210266, -0.9371803998947144, 0.8318963050842285, 1.3966693878173828, 0.9706752300262451]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [-0.06274966895580292, -0.16386201977729797, -0.13042548298835754, -0.28939491510391235, -0.814740777015686]\n",
            "    Token 4:\n",
            "      Initial:   [0.7080122232437134, 0.581470787525177, 0.06049448251724243, 0.5933913588523865, 0.14427685737609863]\n",
            "      Positional:[-0.04879027605056763, -0.07217282056808472, 1.0140752792358398, 0.894528865814209, 0.5336952209472656]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.09197244048118591, -0.24611753225326538, -0.15885694324970245, -0.3894209861755371, -1.0940381288528442]\n",
            "    Token 5:\n",
            "      Initial:   [0.8438466787338257, 0.28415608406066895, 0.8149324059486389, 0.8447468876838684, 0.04272198677062988]\n",
            "      Positional:[-0.11507761478424072, 0.567818284034729, 1.8148789405822754, 0.8344045281410217, 0.522147536277771]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.07659713923931122, -0.1983380764722824, -0.15182647109031677, -0.29895463585853577, -0.8516702651977539]\n",
            "    Token 6:\n",
            "      Initial:   [0.895046591758728, 0.7633596062660217, 0.08945727348327637, 0.5553827285766602, 0.20525509119033813]\n",
            "      Positional:[0.615631103515625, 1.7235298156738281, 1.0366054773330688, 0.23458632826805115, 0.7698975801467896]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.061617251485586166, -0.21555443108081818, -0.12721654772758484, -0.3484081029891968, -1.0567063093185425]\n",
            "\n",
            "Window 1 (from document 1):\n",
            "  Original tokens: [2, 4, 5, 6, 9, 10, 11, 12, 13, 14]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> This is a much longer sentence that will be'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.6751480102539062, 0.8457770347595215, 0.4609410762786865, 0.23101294040679932, 0.16123050451278687]\n",
            "      Positional:[0.6751480102539062, 1.8457770347595215, 0.4609410762786865, 1.2310129404067993, 0.16123050451278687]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.05897480621933937, -0.24791716039180756, -0.08506904542446136, -0.3356645405292511, -1.278488278388977]\n",
            "    Token 1:\n",
            "      Initial:   [0.3449767231941223, 0.09905058145523071, 0.39199894666671753, 0.40931808948516846, 0.5367186069488525]\n",
            "      Positional:[1.1864476203918457, 0.6393529176712036, 0.7029825448989868, 1.3597333431243896, 0.6365520358085632]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.04143408313393593, -0.20822952687740326, -0.08027384430170059, -0.37016040086746216, -1.1620668172836304]\n",
            "    Token 2:\n",
            "      Initial:   [0.19653385877609253, 0.7266424298286438, 0.29172223806381226, 0.4741339087486267, 0.14845764636993408]\n",
            "      Positional:[1.105831265449524, 0.3104955852031708, 0.8828493356704712, 1.280712366104126, 0.34712696075439453]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.07501734048128128, -0.19700343906879425, -0.0028091231361031532, -0.32733944058418274, -1.1182903051376343]\n",
            "    Token 3:\n",
            "      Initial:   [0.26113611459732056, 0.29279088973999023, 0.546219527721405, 0.6296166181564331, 0.452853798866272]\n",
            "      Positional:[0.4022561311721802, -0.6972016096115112, 1.3588683605194092, 1.2123702764511108, 0.7483739852905273]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [-0.014146309345960617, -0.1607431173324585, -0.1677618771791458, -0.34928229451179504, -0.8091157078742981]\n",
            "    Token 4:\n",
            "      Initial:   [0.2864852547645569, 0.3939793109893799, 0.1417897343635559, 0.3650050759315491, 0.0908435583114624]\n",
            "      Positional:[-0.4703172445297241, -0.25966429710388184, 1.0953705310821533, 0.6661425828933716, 0.4802619218826294]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.044495098292827606, -0.2968721389770508, -0.0770031288266182, -0.3284560739994049, -1.031531572341919]\n",
            "    Token 5:\n",
            "      Initial:   [0.6075112819671631, 0.3988390564918518, 0.7365244626998901, 0.6829202771186829, 0.04994887113571167]\n",
            "      Positional:[-0.3514130115509033, 0.6825012564659119, 1.7364709377288818, 0.6725779175758362, 0.5293744206428528]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.0358189158141613, -0.22568048536777496, -0.14055950939655304, -0.37441757321357727, -0.9043295383453369]\n",
            "    Token 6:\n",
            "      Initial:   [0.533362090587616, 0.865123450756073, 0.15184283256530762, 0.7000179886817932, 0.5595780611038208]\n",
            "      Positional:[0.25394660234451294, 1.825293779373169, 1.0989910364151, 0.3792215883731842, 1.124220609664917]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.04461277648806572, -0.24875648319721222, -0.120994932949543, -0.33302462100982666, -0.963486909866333]\n",
            "    Token 7:\n",
            "      Initial:   [0.8307802081108093, 0.42513060569763184, 0.38372546434402466, 0.6006407737731934, 0.891075849533081]\n",
            "      Positional:[1.4877667427062988, 1.179032802581787, 1.1841471195220947, 0.001203298568725586, 1.5352935791015625]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.044502537697553635, -0.1510377824306488, -0.13053949177265167, -0.289414644241333, -0.8521628975868225]\n",
            "    Token 8:\n",
            "      Initial:   [0.5273762345314026, 0.44497090578079224, 0.06080591678619385, 0.9236291646957397, 0.4810645580291748]\n",
            "      Positional:[1.5167344808578491, 0.2994708716869354, 0.6351237297058105, 0.10499674081802368, 1.1984206438064575]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.008502641692757607, -0.12942224740982056, -0.02764608897268772, -0.2297443449497223, -0.9288015365600586]\n",
            "    Token 9:\n",
            "      Initial:   [0.6671133041381836, 0.8943154811859131, 0.6247747540473938, 0.8481590747833252, 0.8240191340446472]\n",
            "      Positional:[1.0792317390441895, -0.016814768314361572, 0.9160339832305908, -0.10848510265350342, 1.6073460578918457]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.021138494834303856, -0.16454915702342987, -0.02442428097128868, -0.26272842288017273, -1.0660018920898438]\n",
            "\n",
            "Window 2 (from document 1):\n",
            "  Original tokens: [2, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> will be split into multiple windows. Observe the term'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.8210680484771729, 0.4615821838378906, 0.12695103883743286, 0.8566694259643555, 0.28137218952178955]\n",
            "      Positional:[0.8210680484771729, 1.4615821838378906, 0.12695103883743286, 1.8566694259643555, 0.28137218952178955]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [-0.11742296069860458, -0.40891775488853455, -0.1991260051727295, -0.5391674637794495, -1.198445439338684]\n",
            "    Token 1:\n",
            "      Initial:   [0.5757255554199219, 0.04680943489074707, 0.8569324612617493, 0.5457733869552612, 0.5099225044250488]\n",
            "      Positional:[1.41719651222229, 0.58711177110672, 1.1679160594940186, 1.4961886405944824, 0.6097559332847595]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.0016385979251936078, -0.10906872153282166, -0.09352782368659973, -0.41637954115867615, -1.1174590587615967]\n",
            "    Token 2:\n",
            "      Initial:   [0.8892160654067993, 0.6698834300041199, 0.9006770253181458, 0.47661054134368896, 0.054286420345306396]\n",
            "      Positional:[1.798513412475586, 0.25373658537864685, 1.4918041229248047, 1.2831889390945435, 0.25295573472976685]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [-0.039076097309589386, -0.16542696952819824, -0.07971913367509842, -0.3959234654903412, -1.1764273643493652]\n",
            "    Token 3:\n",
            "      Initial:   [0.307956337928772, 0.804539144039154, 0.339333713054657, 0.48338866233825684, 0.3095434904098511]\n",
            "      Positional:[0.4490763545036316, -0.1854533553123474, 1.1519825458526611, 1.0661423206329346, 0.6050636768341064]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [-0.008874254301190376, -0.10852587223052979, -0.1191474124789238, -0.3413870632648468, -0.8068032264709473]\n",
            "    Token 4:\n",
            "      Initial:   [0.7281942367553711, 0.06118732690811157, 0.573623776435852, 0.5086588263511658, 0.6320651173591614]\n",
            "      Positional:[-0.028608262538909912, -0.5924562811851501, 1.5272045135498047, 0.8097963333129883, 1.0214834213256836]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.020059609785676003, -0.047972384840250015, -0.07892750203609467, -0.2304529994726181, -0.6748599410057068]\n",
            "    Token 5:\n",
            "      Initial:   [0.5171849727630615, 0.6197429895401001, 0.43795639276504517, 0.9034672975540161, 0.10904985666275024]\n",
            "      Positional:[-0.4417393207550049, 0.9034051895141602, 1.4379029273986816, 0.8931249380111694, 0.5884754061698914]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06469805538654327, -0.16679756343364716, -0.0756797268986702, -0.3218157887458801, -1.0877233743667603]\n",
            "    Token 6:\n",
            "      Initial:   [0.46545302867889404, 0.9254813194274902, 0.7926220893859863, 0.09988802671432495, 0.5020683407783508]\n",
            "      Positional:[0.18603754043579102, 1.8856515884399414, 1.7397702932357788, -0.22090837359428406, 1.0667108297348022]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.04427375644445419, -0.06367484480142593, -0.08797182142734528, -0.25728899240493774, -0.8494876027107239]\n",
            "    Token 7:\n",
            "      Initial:   [0.6467833518981934, 0.16154295206069946, 0.7046706080436707, 0.06812530755996704, 0.2361149787902832]\n",
            "      Positional:[1.3037699460983276, 0.9154452085494995, 1.5050921440124512, -0.5313121676445007, 0.8803326487541199]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.03717530518770218, -0.10442055016756058, -0.017984099686145782, -0.2106507122516632, -0.9510922431945801]\n",
            "    Token 8:\n",
            "      Initial:   [0.49696773290634155, 0.10285913944244385, 0.19554924964904785, 0.4885968565940857, 0.94418865442276]\n",
            "      Positional:[1.486325979232788, -0.042640894651412964, 0.7698670625686646, -0.33003556728363037, 1.6615447998046875]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.07276628911495209, -0.17084871232509613, -0.05871192738413811, -0.23200617730617523, -1.0555654764175415]\n",
            "    Token 9:\n",
            "      Initial:   [0.7575096487998962, 0.024004697799682617, 0.030164659023284912, 0.1162298321723938, 0.6563928127288818]\n",
            "      Positional:[1.1696281433105469, -0.887125551700592, 0.32142388820648193, -0.8404143452644348, 1.4397196769714355]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.14221516251564026, -0.1686999350786209, -0.053729698061943054, -0.18013548851013184, -0.8759494423866272]\n",
            "\n",
            "Window 3 (from document 1):\n",
            "  Original tokens: [2, 20, 21, 22, 3, 0, 0, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "  Decoded: '<SOS> the term overlapping? <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.4426397681236267, 0.1410609483718872, 0.24916547536849976, 0.5636696219444275, 0.7930962443351746]\n",
            "      Positional:[0.4426397681236267, 1.1410609483718872, 0.24916547536849976, 1.5636696815490723, 0.7930962443351746]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "    Token 1:\n",
            "      Initial:   [0.09352701902389526, 0.3645132780075073, 0.9416290521621704, 0.6954596638679504, 0.440715491771698]\n",
            "      Positional:[0.9349979758262634, 0.9048156142234802, 1.252612590789795, 1.6458749771118164, 0.5405489206314087]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.18640053272247314, -0.11264309287071228, -0.10368893295526505, -0.40958544611930847, -0.8982743620872498]\n",
            "    Token 2:\n",
            "      Initial:   [0.9064120650291443, 0.7904268503189087, 0.5280411243438721, 0.33811360597610474, 0.6275352239608765]\n",
            "      Positional:[1.8157094717025757, 0.37428000569343567, 1.1191682815551758, 1.1446919441223145, 0.8262045383453369]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.07067184895277023, -0.047992922365665436, -0.09928382188081741, -0.37957650423049927, -0.8848634958267212]\n",
            "    Token 3:\n",
            "      Initial:   [0.5291450023651123, 0.9332912564277649, 0.5669952630996704, 0.18366146087646484, 0.3201432228088379]\n",
            "      Positional:[0.6702650189399719, -0.05670124292373657, 1.3796441555023193, 0.7664151191711426, 0.6156634092330933]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [-0.014089025557041168, -0.05852275341749191, -0.15473130345344543, -0.3373975157737732, -0.7452107667922974]\n",
            "    Token 4:\n",
            "      Initial:   [0.5357943773269653, 0.8231447339057922, 0.717495322227478, 0.5642438530921936, 0.6536393165588379]\n",
            "      Positional:[-0.22100812196731567, 0.16950112581253052, 1.6710760593414307, 0.8653813600540161, 1.0430576801300049]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.07988683134317398, -0.08763953298330307, -0.027796998620033264, -0.26068103313446045, -0.9484126567840576]\n",
            "\n",
            "Window 4 (from document 2):\n",
            "  Original tokens: [2, 23, 11, 24, 25, 26, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> Another sentence of medium length. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.7841647267341614, 0.5840846300125122, 0.19909238815307617, 0.22848153114318848, 0.8982774019241333]\n",
            "      Positional:[0.7841647267341614, 1.5840846300125122, 0.19909238815307617, 1.2284815311431885, 0.8982774019241333]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.03987874835729599, 0.03667226806282997, -0.030083656311035156, -0.2770591378211975, -1.2762231826782227]\n",
            "    Token 1:\n",
            "      Initial:   [0.8267073631286621, 0.020554959774017334, 0.34274357557296753, 0.06407266855239868, 0.27571189403533936]\n",
            "      Positional:[1.6681783199310303, 0.5608572959899902, 0.6537271738052368, 1.0144879817962646, 0.37554532289505005]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.032375186681747437, 0.029307974502444267, -0.03673625364899635, -0.23817864060401917, -1.2367359399795532]\n",
            "    Token 2:\n",
            "      Initial:   [0.3559921979904175, 0.14557039737701416, 0.26450616121292114, 0.770724356174469, 0.8221427798271179]\n",
            "      Positional:[1.265289545059204, -0.27057644724845886, 0.8556332588195801, 1.5773026943206787, 1.0208121538162231]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [-0.05338336527347565, 0.012570420280098915, -0.15115241706371307, -0.24879567325115204, -0.8549836277961731]\n",
            "    Token 3:\n",
            "      Initial:   [0.9135463833808899, 0.29690635204315186, 0.26972341537475586, 0.31535130739212036, 0.030896008014678955]\n",
            "      Positional:[1.0546663999557495, -0.6930861473083496, 1.0823723077774048, 0.8981049656867981, 0.3264162242412567]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [0.005573996342718601, -0.015436733141541481, -0.052969980984926224, -0.24044989049434662, -1.147167682647705]\n",
            "    Token 4:\n",
            "      Initial:   [0.4374884366989136, 0.7889836430549622, 0.9846795201301575, 0.4369194507598877, 0.862311601638794]\n",
            "      Positional:[-0.31931406259536743, 0.13534003496170044, 1.9382603168487549, 0.7380568981170654, 1.251729965209961]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.07932911813259125, -0.04979824274778366, -0.04533667117357254, -0.22946232557296753, -1.092319369316101]\n",
            "    Token 5:\n",
            "      Initial:   [0.47835153341293335, 0.2595415711402893, 0.864340603351593, 0.523161768913269, 0.6052607297897339]\n",
            "      Positional:[-0.48057276010513306, 0.5432037711143494, 1.8642871379852295, 0.5128194093704224, 1.084686279296875]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06377560645341873, -0.06765666604042053, -0.047441694885492325, -0.22163477540016174, -1.0969778299331665]\n",
            "    Token 6:\n",
            "      Initial:   [0.009778320789337158, 0.785380482673645, 0.8621267676353455, 0.356050968170166, 0.9635248184204102]\n",
            "      Positional:[-0.26963716745376587, 1.7455507516860962, 1.8092749118804932, 0.03525456786155701, 1.5281672477722168]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06873965263366699, -0.07786945253610611, 0.0034874873235821724, -0.21030253171920776, -0.9457250833511353]\n",
            "\n",
            "--- End of Expanded Test ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Feed Forward Netwrok\n",
        "\n",
        "A feed-forward network is a multi-layered structure in which information moves in a single direction, from the input layer to the output layer.\n",
        "\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   Transformer Feed-Forward Layers Are Key-Value Memories\n",
        " [Link-ArXiv](https://arxiv.org/abs/2012.14913)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AGmlNshUAmmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        #######################Code Here###############################\n",
        "        # Define feed-forward network using nn.Sequential\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "          # First linear layer\n",
        "          # ReLU activation\n",
        "          # Dropout for regularization\n",
        "          # Second linear layer\n",
        "        )\n",
        "        ###############################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # Apply the feed-forward network"
      ],
      "metadata": {
        "id": "yH4rocZtFSY7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Feed Forward Layer"
      ],
      "metadata": {
        "id": "XxPG41aLFJz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_all_transformer_components():\n",
        "    print(\"\\n--- Testing Transformer Components ---\\n\")\n",
        "\n",
        "    # Set a fixed seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Sample texts\n",
        "    texts = [\n",
        "        \"This is a short sentence.\",\n",
        "        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n",
        "        \"Another sentence of medium length.\"\n",
        "    ]\n",
        "\n",
        "    # Initialize tokenizer and fit it to the texts\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    tokenizer.fit(texts)\n",
        "\n",
        "    # Create dataset\n",
        "    max_length = 10\n",
        "    overlap = 2\n",
        "    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n",
        "\n",
        "    # Hyperparameters\n",
        "    d_model = 16  # Small dimension for demonstration\n",
        "    nhead = 2\n",
        "    d_ff = d_model  # Feed-forward dimension\n",
        "    dropout = 0.1\n",
        "\n",
        "    # Initialize components\n",
        "    pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "    masked_self_attn = MaskedAttention(d_model, nhead)\n",
        "    feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    print(f\"Dataset configuration:\")\n",
        "    print(f\"  Max length: {max_length}\")\n",
        "    print(f\"  Overlap: {overlap}\")\n",
        "    print(f\"  Total windows: {len(dataset)}\")\n",
        "    print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
        "    print(f\"  Embedding dimension: {d_model}\")\n",
        "    print(f\"  Number of attention heads: {nhead}\")\n",
        "    print(f\"  Feed-forward dimension: {d_ff}\\n\")\n",
        "\n",
        "    # Process each window\n",
        "    for i in range(len(dataset)):\n",
        "        tokens, attention_mask, doc_idx = dataset[i]\n",
        "\n",
        "        print(f\"Window {i} (from document {doc_idx}):\")\n",
        "        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n",
        "        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n",
        "        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n",
        "\n",
        "        # Convert tokens to \"embeddings\" (just for demonstration)\n",
        "        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n",
        "        print(f\"  Shape of pseudo embeddings: {pseudo_embeddings.shape}\")\n",
        "\n",
        "        # Apply positional encoding\n",
        "        pos_encoded = pos_encoder(pseudo_embeddings)\n",
        "        print(f\"  Shape after positional encoding: {pos_encoded.shape}\")\n",
        "\n",
        "        # Apply masked self-attention\n",
        "        attn_output, attn_mask = masked_self_attn(pos_encoded)\n",
        "        print(f\"  Shape after masked self-attention: {attn_output.shape}\")\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        ff_output = feed_forward(attn_output)\n",
        "        print(f\"  Shape after feed-forward: {ff_output.shape}\")\n",
        "\n",
        "        # Display the effect of positional encoding, attention, and feed-forward for all tokens\n",
        "        print(f\"  Transformer effect on tokens:\")\n",
        "        for j in range(tokens.size(1)):\n",
        "            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n",
        "                print(f\"    Token {j}:\")\n",
        "                print(f\"      Initial:   {pseudo_embeddings[0, j, :5].tolist()}\")\n",
        "                print(f\"      Positional:{pos_encoded[0, j, :5].tolist()}\")\n",
        "                print(f\"      Attention Mask: {attn_mask[j, :5].tolist()}\")  # Show first 5 values\n",
        "                print(f\"      Attention: {attn_output[0, j, :5].tolist()}\")\n",
        "                print(f\"      Feed-Forward: {ff_output[0, j, :5].tolist()}\")\n",
        "        print()\n",
        "\n",
        "    print(\"--- End of Expanded Test ---\")\n",
        "\n",
        "# Run the expanded test\n",
        "test_all_transformer_components()"
      ],
      "metadata": {
        "id": "FyyrKkJZAuMA",
        "outputId": "c6714821-9f30-4f08-ec53-233a9b25c44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Transformer Components ---\n",
            "\n",
            "Dataset configuration:\n",
            "  Max length: 10\n",
            "  Overlap: 2\n",
            "  Total windows: 5\n",
            "  Vocabulary size: 27\n",
            "  Embedding dimension: 16\n",
            "  Number of attention heads: 2\n",
            "  Feed-forward dimension: 16\n",
            "\n",
            "Window 0 (from document 0):\n",
            "  Original tokens: [2, 4, 5, 6, 7, 8, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> This is a short sentence. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Shape after feed-forward: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.6404210329055786, 0.8828725218772888, 0.387043297290802, 0.5357943773269653, 0.8231447339057922]\n",
            "      Positional:[0.6404210329055786, 1.8828725814819336, 0.387043297290802, 1.5357943773269653, 0.8231447339057922]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [-0.050738587975502014, -0.3658566176891327, -0.18821901082992554, -0.5680540204048157, -1.113892912864685]\n",
            "      Feed-Forward: [0.02904241904616356, 0.18689507246017456, 0.5315297842025757, 0.2933975160121918, 0.7633250951766968]\n",
            "    Token 1:\n",
            "      Initial:   [0.24249053001403809, 0.9345868229866028, 0.9304925799369812, 0.4808894991874695, 0.09874528646469116]\n",
            "      Positional:[1.0839614868164062, 1.4748891592025757, 1.2414761781692505, 1.4313048124313354, 0.19857871532440186]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.1480904370546341, -0.20995332300662994, -0.0980050265789032, -0.4300519824028015, -1.0556870698928833]\n",
            "      Feed-Forward: [0.06906585395336151, 0.16978099942207336, 0.5372937917709351, 0.2833501100540161, 0.7439247369766235]\n",
            "    Token 2:\n",
            "      Initial:   [0.5460624694824219, 0.8276379704475403, 0.9543738961219788, 0.12177222967147827, 0.7099605798721313]\n",
            "      Positional:[1.455359935760498, 0.41149112582206726, 1.5455009937286377, 0.9283506274223328, 0.9086298942565918]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.10684087127447128, -0.18199558556079865, -0.08497927337884903, -0.3524727523326874, -0.9943309426307678]\n",
            "      Feed-Forward: [-0.10594794154167175, 0.1782640963792801, 0.4242222011089325, 0.13139745593070984, 0.5549695491790771]\n",
            "    Token 3:\n",
            "      Initial:   [0.11537909507751465, 0.2047392725944519, 0.29425084590911865, 0.8002946376800537, 0.6636282205581665]\n",
            "      Positional:[0.25649911165237427, -0.7852532267570496, 1.1068997383117676, 1.3830482959747314, 0.9591484069824219]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [0.08654603362083435, -0.17604249715805054, -0.09159720689058304, -0.32893192768096924, -0.9593493342399597]\n",
            "      Feed-Forward: [0.0625506117939949, 0.1851588785648346, 0.5221771001815796, 0.2614033818244934, 0.7050285339355469]\n",
            "    Token 4:\n",
            "      Initial:   [0.04842644929885864, 0.9460888504981995, 0.2638719081878662, 0.7417299151420593, 0.10241413116455078]\n",
            "      Positional:[-0.7083760499954224, 0.29244524240493774, 1.2174526453018188, 1.0428674221038818, 0.4918324947357178]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.15354861319065094, -0.17434829473495483, -0.052772991359233856, -0.3065394461154938, -0.9968476295471191]\n",
            "      Feed-Forward: [0.07576026767492294, 0.17118604481220245, 0.49867716431617737, 0.25073933601379395, 0.6986097097396851]\n",
            "    Token 5:\n",
            "      Initial:   [0.08954626321792603, 0.8426312208175659, 0.20782166719436646, 0.8937427401542664, 0.5548718571662903]\n",
            "      Positional:[-0.8693780303001404, 1.126293420791626, 1.207768201828003, 0.8834003806114197, 1.0342974662780762]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06234590336680412, -0.15164215862751007, -0.09404868632555008, -0.2328084260225296, -0.7058144211769104]\n",
            "      Feed-Forward: [0.041834332048892975, 0.14446614682674408, 0.3626992106437683, 0.18833939731121063, 0.5624408721923828]\n",
            "    Token 6:\n",
            "      Initial:   [0.450192391872406, 0.48418259620666504, 0.4626423120498657, 0.9172412753105164, 0.7841647267341614]\n",
            "      Positional:[0.17077690362930298, 1.4443528652191162, 1.4097905158996582, 0.596444845199585, 1.3488072156906128]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.11785881221294403, -0.1726151555776596, -0.08883893489837646, -0.3262043595314026, -0.8404727578163147]\n",
            "      Feed-Forward: [0.21829122304916382, 0.07398951798677444, 0.2988884150981903, -0.010637279599905014, 0.5460066795349121]\n",
            "\n",
            "Window 1 (from document 1):\n",
            "  Original tokens: [2, 4, 5, 6, 9, 10, 11, 12, 13, 14]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> This is a much longer sentence that will be'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Shape after feed-forward: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.8482050895690918, 0.1713215708732605, 0.4374884366989136, 0.7889836430549622, 0.9846795201301575]\n",
            "      Positional:[0.8482050895690918, 1.1713216304779053, 0.4374884366989136, 1.7889835834503174, 0.9846795201301575]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.26172947883605957, 0.044291771948337555, 0.0029501705430448055, -0.24750620126724243, -1.3034673929214478]\n",
            "      Feed-Forward: [0.11478757113218307, 0.12962555885314941, 0.6214985847473145, 0.2833414673805237, 0.7594414949417114]\n",
            "    Token 1:\n",
            "      Initial:   [0.5464029908180237, 0.19689732789993286, 0.47835153341293335, 0.2595415711402893, 0.864340603351593]\n",
            "      Positional:[1.387873888015747, 0.7371996641159058, 0.7893351316452026, 1.2099568843841553, 0.9641740322113037]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.11392708122730255, 0.1123168095946312, -0.037552185356616974, -0.11448896676301956, -0.5738725662231445]\n",
            "      Feed-Forward: [0.007524106651544571, 0.1146254688501358, 0.1582605093717575, 0.046741269528865814, 0.42615586519241333]\n",
            "    Token 2:\n",
            "      Initial:   [0.14241832494735718, 0.41703855991363525, 0.009778320789337158, 0.785380482673645, 0.8621267676353455]\n",
            "      Positional:[1.0517157316207886, 0.0008917152881622314, 0.6009054183959961, 1.5919588804244995, 1.0607961416244507]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [-0.015577645972371101, -0.058903004974126816, -0.08418192714452744, -0.38527798652648926, -1.1194465160369873]\n",
            "      Feed-Forward: [0.05052485316991806, 0.16949300467967987, 0.5485360622406006, 0.2626594305038452, 0.6829840540885925]\n",
            "    Token 3:\n",
            "      Initial:   [0.831032931804657, 0.5802402496337891, 0.27006202936172485, 0.606012761592865, 0.5013369917869568]\n",
            "      Positional:[0.9721529483795166, -0.4097522497177124, 1.0827109813690186, 1.1887664794921875, 0.7968572378158569]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [-0.08772715926170349, -0.08662807941436768, -0.11382120847702026, -0.27013975381851196, -0.5677108764648438]\n",
            "      Feed-Forward: [0.07098985463380814, 0.08276505023241043, 0.2555062174797058, 0.06761068105697632, 0.30704355239868164]\n",
            "    Token 4:\n",
            "      Initial:   [0.11609572172164917, 0.5971465706825256, 0.612359344959259, 0.4562438726425171, 0.9752312302589417]\n",
            "      Positional:[-0.6407067775726318, -0.056497037410736084, 1.5659401416778564, 0.7573813199996948, 1.3646495342254639]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.00352421379648149, -0.10031091421842575, -0.009389585815370083, -0.3154616057872772, -1.0384835004806519]\n",
            "      Feed-Forward: [0.07413667440414429, 0.1574360728263855, 0.47193366289138794, 0.2240017056465149, 0.6440102458000183]\n",
            "    Token 5:\n",
            "      Initial:   [0.7693662643432617, 0.5967926383018494, 0.9261442422866821, 0.800311803817749, 0.8577843904495239]\n",
            "      Positional:[-0.1895580291748047, 0.8804548382759094, 1.9260907173156738, 0.7899694442749023, 1.337209939956665]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.09619339555501938, -0.030821682885289192, -0.005946798715740442, -0.24919725954532623, -1.051631212234497]\n",
            "      Feed-Forward: [0.06662359088659286, 0.1514822542667389, 0.5009785890579224, 0.23864208161830902, 0.6499640941619873]\n",
            "    Token 6:\n",
            "      Initial:   [0.9680942893028259, 0.5217174887657166, 0.8062544465065002, 0.07799309492111206, 0.2423895001411438]\n",
            "      Positional:[0.6886788010597229, 1.4818878173828125, 1.7534027099609375, -0.24280330538749695, 0.8070319890975952]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.04535919055342674, -0.054221466183662415, -0.18894393742084503, -0.3103330433368683, -0.5672949552536011]\n",
            "      Feed-Forward: [-0.017254501581192017, 0.14402534067630768, 0.31267836689949036, 0.17529653012752533, 0.4738779366016388]\n",
            "    Token 7:\n",
            "      Initial:   [0.2282291054725647, 0.6611340641975403, 0.7934814095497131, 0.1113814115524292, 0.7558900117874146]\n",
            "      Positional:[0.885215699672699, 1.4150363206863403, 1.5939030647277832, -0.4880560636520386, 1.4001076221466064]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.002616190817207098, -0.04760092869400978, -0.06678453087806702, -0.3081777095794678, -0.83346027135849]\n",
            "      Feed-Forward: [0.06308744102716446, 0.11440922319889069, 0.39513665437698364, 0.27062907814979553, 0.5870636701583862]\n",
            "    Token 8:\n",
            "      Initial:   [0.843437910079956, 0.6954054832458496, 0.17659258842468262, 0.6299582719802856, 0.11878162622451782]\n",
            "      Positional:[1.8327960968017578, 0.5499054193496704, 0.7509104013442993, -0.18867415189743042, 0.8361377120018005]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.02815011516213417, -0.09026066958904266, -0.03763381019234657, -0.26131948828697205, -0.9426624178886414]\n",
            "      Feed-Forward: [0.041539110243320465, 0.1564626544713974, 0.42421984672546387, 0.2008833885192871, 0.5804324150085449]\n",
            "    Token 9:\n",
            "      Initial:   [0.5381041765213013, 0.38817089796066284, 0.1730700135231018, 0.34045058488845825, 0.11565500497817993]\n",
            "      Positional:[0.9502226710319519, -0.5229593515396118, 0.46432924270629883, -0.6161935925483704, 0.8989818692207336]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.006130640394985676, -0.09568147361278534, -0.018422286957502365, -0.22265085577964783, -0.9483146667480469]\n",
            "      Feed-Forward: [0.020200777798891068, 0.1365257203578949, 0.2718375325202942, 0.16412165760993958, 0.4682806432247162]\n",
            "\n",
            "Window 2 (from document 1):\n",
            "  Original tokens: [2, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded: '<SOS> will be split into multiple windows. Observe the term'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Shape after feed-forward: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.48279356956481934, 0.11034709215164185, 0.8470419645309448, 0.11604011058807373, 0.9668178558349609]\n",
            "      Positional:[0.48279356956481934, 1.110347032546997, 0.8470419645309448, 1.1160401105880737, 0.9668178558349609]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.1984911412000656, -0.31055963039398193, 0.23321853578090668, -0.04764890298247337, -1.1768109798431396]\n",
            "      Feed-Forward: [0.009499412029981613, 0.02941291779279709, 0.25512608885765076, 0.18731360137462616, 0.5583078861236572]\n",
            "    Token 1:\n",
            "      Initial:   [0.5593568682670593, 0.2496773600578308, 0.9859225153923035, 0.5373151898384094, 0.8382884860038757]\n",
            "      Positional:[1.4008278846740723, 0.7899796962738037, 1.2969061136245728, 1.4877305030822754, 0.9381219148635864]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.1560593694448471, 0.12220688164234161, -0.08168131858110428, -0.18269892036914825, -0.6693078279495239]\n",
            "      Feed-Forward: [0.038386858999729156, 0.09353901445865631, 0.31452175974845886, 0.17094306647777557, 0.5387357473373413]\n",
            "    Token 2:\n",
            "      Initial:   [0.5872496962547302, 0.184309184551239, 0.5963478684425354, 0.01723182201385498, 0.5796757340431213]\n",
            "      Positional:[1.4965471029281616, -0.231837660074234, 1.1874749660491943, 0.8238102197647095, 0.7783450484275818]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.0672667846083641, 0.02014060877263546, -0.07840873301029205, -0.19345533847808838, -0.9385929703712463]\n",
            "      Feed-Forward: [-0.03164586424827576, 0.12949058413505554, 0.3536367416381836, 0.12153390049934387, 0.4947638511657715]\n",
            "    Token 3:\n",
            "      Initial:   [0.479445219039917, 0.24807840585708618, 0.6794151663780212, 0.015504121780395508, 0.6769341230392456]\n",
            "      Positional:[0.6205652356147766, -0.7419140934944153, 1.4920639991760254, 0.5982577800750732, 0.972454309463501]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [0.07471844553947449, -0.13412590324878693, -0.037656497210264206, -0.20672200620174408, -1.2463480234146118]\n",
            "      Feed-Forward: [0.09366832673549652, 0.14939957857131958, 0.5047436952590942, 0.3102390468120575, 0.7258061170578003]\n",
            "    Token 4:\n",
            "      Initial:   [0.6771438717842102, 0.22949224710464478, 0.330100417137146, 0.7359793186187744, 0.7225840091705322]\n",
            "      Positional:[-0.0796586275100708, -0.42415136098861694, 1.2836811542510986, 1.0371167659759521, 1.1120023727416992]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.14710567891597748, -0.09391296654939651, 0.03747010976076126, -0.12497495114803314, -0.8216280937194824]\n",
            "      Feed-Forward: [0.060052283108234406, 0.13360291719436646, 0.36868730187416077, 0.18909353017807007, 0.556367814540863]\n",
            "    Token 5:\n",
            "      Initial:   [0.963742733001709, 0.6310474276542664, 0.795417845249176, 0.38694608211517334, 0.862110435962677]\n",
            "      Positional:[0.004818439483642578, 0.9147096276283264, 1.7953643798828125, 0.37660375237464905, 1.341536045074463]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.12974628806114197, -0.09687595069408417, -0.05369620397686958, -0.22609099745750427, -1.0977604389190674]\n",
            "      Feed-Forward: [0.046555958688259125, 0.17380759119987488, 0.4821860194206238, 0.23048461973667145, 0.6649907231330872]\n",
            "    Token 6:\n",
            "      Initial:   [0.00911945104598999, 0.2707793116569519, 0.5036577582359314, 0.5449010729789734, 0.2741878032684326]\n",
            "      Positional:[-0.27029603719711304, 1.2309496402740479, 1.450805902481079, 0.22410467267036438, 0.838830292224884]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06019969657063484, -0.12003613263368607, -0.12429291009902954, -0.2616177797317505, -0.8774986863136292]\n",
            "      Feed-Forward: [-0.010657228529453278, 0.1476866751909256, 0.24163323640823364, 0.16702161729335785, 0.4776756465435028]\n",
            "    Token 7:\n",
            "      Initial:   [0.2728124260902405, 0.36394184827804565, 0.1941431164741516, 0.31368112564086914, 0.3072052001953125]\n",
            "      Positional:[0.9297990202903748, 1.1178441047668457, 0.9945647120475769, -0.28575634956359863, 0.9514228701591492]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.036008071154356, -0.09912536293268204, -0.05992232263088226, -0.18036264181137085, -0.7753729224205017]\n",
            "      Feed-Forward: [0.022233814001083374, 0.14993871748447418, 0.3527628183364868, 0.1725548356771469, 0.5268583297729492]\n",
            "    Token 8:\n",
            "      Initial:   [0.38788557052612305, 0.01776754856109619, 0.5189154148101807, 0.3234051465988159, 0.9339582920074463]\n",
            "      Positional:[1.3772437572479248, -0.12773248553276062, 1.0932332277297974, -0.49522727727890015, 1.651314377784729]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.00879715196788311, -0.09659191220998764, -0.0901007130742073, -0.20273306965827942, -0.9349209070205688]\n",
            "      Feed-Forward: [-0.034871965646743774, 0.18038910627365112, 0.29730257391929626, 0.18499241769313812, 0.43359050154685974]\n",
            "    Token 9:\n",
            "      Initial:   [0.695349395275116, 0.21589624881744385, 0.6514241695404053, 0.7520686984062195, 0.05589967966079712]\n",
            "      Positional:[1.1074678897857666, -0.6952340006828308, 0.9426833987236023, -0.20457547903060913, 0.8392265439033508]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [-0.020792366936802864, -0.1633158177137375, 0.001002727192826569, -0.14995191991329193, -1.0879775285720825]\n",
            "      Feed-Forward: [-0.021422766149044037, 0.17816288769245148, 0.3727238178253174, 0.11799505352973938, 0.5998784303665161]\n",
            "\n",
            "Window 3 (from document 1):\n",
            "  Original tokens: [2, 20, 21, 22, 3, 0, 0, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "  Decoded: '<SOS> the term overlapping? <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Shape after feed-forward: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.0720055103302002, 0.670230507850647, 0.9359629154205322, 0.44709205627441406, 0.8521108627319336]\n",
            "      Positional:[0.0720055103302002, 1.670230507850647, 0.9359629154205322, 1.447092056274414, 0.8521108627319336]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.17267076671123505, 0.1858724057674408, -0.23046626150608063, -0.3413489758968353, -0.06833746284246445]\n",
            "      Feed-Forward: [-0.03590485453605652, 0.04043193161487579, 0.13883115351200104, 0.15921704471111298, 0.17060108482837677]\n",
            "    Token 1:\n",
            "      Initial:   [0.663816511631012, 0.7214484214782715, 0.6147032380104065, 0.11302351951599121, 0.18708372116088867]\n",
            "      Positional:[1.5052874088287354, 1.2617506980895996, 0.9256868362426758, 1.063438892364502, 0.28691715002059937]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.11617398262023926, 0.10444784164428711, -0.10511089861392975, -0.24646404385566711, -0.6642942428588867]\n",
            "      Feed-Forward: [0.014247387647628784, 0.08941090852022171, 0.32400527596473694, 0.2716605067253113, 0.4944254159927368]\n",
            "    Token 2:\n",
            "      Initial:   [0.673240602016449, 0.9749525189399719, 0.8300654888153076, 0.746844470500946, 0.39827436208724976]\n",
            "      Positional:[1.5825380086898804, 0.5588057041168213, 1.4211926460266113, 1.5534229278564453, 0.5969436764717102]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.04487333819270134, -0.1340511590242386, 0.07215677946805954, -0.18665170669555664, -1.1672117710113525]\n",
            "      Feed-Forward: [-0.016347840428352356, 0.13307899236679077, 0.2837817072868347, 0.1970851868391037, 0.528986930847168]\n",
            "    Token 3:\n",
            "      Initial:   [0.6000575423240662, 0.047689735889434814, 0.2444155216217041, 0.616789698600769, 0.3503468632698059]\n",
            "      Positional:[0.7411775588989258, -0.9423027634620667, 1.057064414024353, 1.1995433568954468, 0.645867109298706]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [0.0014423959655687213, -0.1027490571141243, 0.00014713828568346798, -0.23220431804656982, -1.0746171474456787]\n",
            "      Feed-Forward: [-0.0072144195437431335, 0.16640067100524902, 0.46187475323677063, 0.1560441106557846, 0.5248229503631592]\n",
            "    Token 4:\n",
            "      Initial:   [0.16889286041259766, 0.7045608162879944, 0.4396747946739197, 0.33986175060272217, 0.07609409093856812]\n",
            "      Positional:[-0.5879096388816833, 0.050917208194732666, 1.3932554721832275, 0.6409991979598999, 0.4655124545097351]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.03227929398417473, -0.14293846487998962, -0.09381403028964996, -0.373908668756485, -1.051520586013794]\n",
            "      Feed-Forward: [0.06141145899891853, 0.09431195259094238, 0.3916475176811218, 0.3620156943798065, 0.5845938920974731]\n",
            "\n",
            "Window 4 (from document 2):\n",
            "  Original tokens: [2, 23, 11, 24, 25, 26, 3, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "  Decoded: '<SOS> Another sentence of medium length. <EOS> <PAD> <PAD> <PAD>'\n",
            "  Shape of pseudo embeddings: torch.Size([1, 10, 16])\n",
            "  Shape after positional encoding: torch.Size([1, 10, 16])\n",
            "  Shape after masked self-attention: torch.Size([1, 10, 16])\n",
            "  Shape after feed-forward: torch.Size([1, 10, 16])\n",
            "  Transformer effect on tokens:\n",
            "    Token 0:\n",
            "      Initial:   [0.8849362730979919, 0.8748436570167542, 0.6705592274665833, 0.044549643993377686, 0.564368486404419]\n",
            "      Positional:[0.8849362730979919, 1.8748435974121094, 0.6705592274665833, 1.0445497035980225, 0.564368486404419]\n",
            "      Attention Mask: [0.0, -inf, -inf, -inf, -inf]\n",
            "      Attention: [0.20878231525421143, -0.15734127163887024, -0.0012268217978999019, -0.2711455523967743, -1.1942527294158936]\n",
            "      Feed-Forward: [0.045432597398757935, 0.16190291941165924, 0.5176410675048828, 0.2658088505268097, 0.696207582950592]\n",
            "    Token 1:\n",
            "      Initial:   [0.6237038373947144, 0.9575220346450806, 0.8101148009300232, 0.6500447988510132, 0.124347984790802]\n",
            "      Positional:[1.4651747941970825, 1.4978244304656982, 1.1210983991622925, 1.6004600524902344, 0.2241814136505127]\n",
            "      Attention Mask: [0.0, 0.0, -inf, -inf, -inf]\n",
            "      Attention: [0.0861927941441536, -0.1952800303697586, 0.12093131989240646, -0.16119571030139923, -1.1908278465270996]\n",
            "      Feed-Forward: [0.14048194885253906, 0.1447025090456009, 0.49392011761665344, 0.21852488815784454, 0.6728893518447876]\n",
            "    Token 2:\n",
            "      Initial:   [0.7535275220870972, 0.24774861335754395, 0.8958203196525574, 0.3929680585861206, 0.3314681053161621]\n",
            "      Positional:[1.6628248691558838, -0.16839823126792908, 1.4869474172592163, 1.199546456336975, 0.5301374197006226]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, -inf, -inf]\n",
            "      Attention: [0.06114288792014122, -0.12861597537994385, -0.021413788199424744, -0.26523250341415405, -1.2565237283706665]\n",
            "      Feed-Forward: [0.035860463976860046, 0.08015449345111847, 0.45669668912887573, 0.3439218997955322, 0.5945427417755127]\n",
            "    Token 3:\n",
            "      Initial:   [0.38317084312438965, 0.0420495867729187, 0.7348012328147888, 0.9096901416778564, 0.9274627566337585]\n",
            "      Positional:[0.5242908596992493, -0.9479429125785828, 1.547450065612793, 1.4924437999725342, 1.2229830026626587]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, -inf]\n",
            "      Attention: [0.02971329540014267, -0.16072015464305878, -0.044811639934778214, -0.2987823784351349, -1.174129843711853]\n",
            "      Feed-Forward: [-0.11656023561954498, 0.189690962433815, 0.4200984537601471, 0.10386031866073608, 0.5404801368713379]\n",
            "    Token 4:\n",
            "      Initial:   [0.33457285165786743, 0.3757324814796448, 0.791743278503418, 0.04166102409362793, 0.8685685992240906]\n",
            "      Positional:[-0.4222296476364136, -0.27791112661361694, 1.7453240156173706, 0.34279850125312805, 1.2579870223999023]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.06338796764612198, -0.17579689621925354, -0.05110440030694008, -0.2873130440711975, -1.0823701620101929]\n",
            "      Feed-Forward: [0.045016296207904816, 0.1874639391899109, 0.4903881251811981, 0.23973819613456726, 0.6729083061218262]\n",
            "    Token 5:\n",
            "      Initial:   [0.5158427953720093, 0.5912436246871948, 0.36049216985702515, 0.1122579574584961, 0.23192840814590454]\n",
            "      Positional:[-0.44308149814605713, 0.8749058246612549, 1.3604387044906616, 0.1019156202673912, 0.7113539576530457]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.035830169916152954, -0.2458183914422989, -0.03277260437607765, -0.2626345455646515, -1.1105313301086426]\n",
            "      Feed-Forward: [0.08762533217668533, 0.14315108954906464, 0.44601792097091675, 0.28356900811195374, 0.7006350755691528]\n",
            "    Token 6:\n",
            "      Initial:   [0.11360877752304077, 0.7837932109832764, 0.08675247430801392, 0.7499111890792847, 0.5592571496963501]\n",
            "      Positional:[-0.16580671072006226, 1.7439634799957275, 1.0339007377624512, 0.42911478877067566, 1.1238996982574463]\n",
            "      Attention Mask: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "      Attention: [0.08969175815582275, -0.16465890407562256, -0.049520786851644516, -0.2900064289569855, -1.109303593635559]\n",
            "      Feed-Forward: [0.04146646335721016, 0.17228227853775024, 0.4749148190021515, 0.23492060601711273, 0.6604798436164856]\n",
            "\n",
            "--- End of Expanded Test ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Decoder Layer\n",
        "\n",
        "Implementation of a Transformer Decoder Layer with Masked Multi-Head Attention and Feed Forward Network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HM696sBjAtmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YPWxAfNbXhJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        #######################Code Here###############################\n",
        "\n",
        "        # Define the decoder - Layer Normalization, Attention, Feed_forward Dropouts\n",
        "\n",
        "        ###############################################################\n",
        "        self.masked_attention = MaskedAttention(d_model, nhead, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Masked Multi-Head Attention\n",
        "        normed_x = self.norm1(x)\n",
        "        attn_output, _ = self.masked_attention(normed_x) # _ because we returned also the mask in the previous demonstration\n",
        "        # Feed-Forward Network\n",
        "        normed_attn = self.norm1(attn_output)\n",
        "        ff_output = self.feed_forward(normed_attn)\n",
        "        #######################Code Here###############################.\n",
        "\n",
        "        # Create the remaining connection and residual connections - What is a residual connection? (https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "        return x + ff_output"
      ],
      "metadata": {
        "id": "P02MAkENBZrS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Decoder-only Transformer\n",
        "\n",
        "The components of a Decoder-Only Transformer include an embedding layer for token representation, positional encoding for sequential information, stacked decoder layers for hierarchical processing, layer normalization for stability, and an output projection layer for generating tokens."
      ],
      "metadata": {
        "id": "VJULRnp4BZJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, nhead: int, num_layers: int,\n",
        "                 d_ff: int, max_seq_length: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, nhead, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: (batch_size, seq_len)\n",
        "\n",
        "        # Embed the input\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Apply decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        output = self.output_projection(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate(self, start_tokens: torch.Tensor, max_length: int,\n",
        "                 temperature: float = 1.0) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        current_seq = start_tokens\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length - start_tokens.size(1)):\n",
        "                # Ensure we're not exceeding the maximum sequence length\n",
        "                if current_seq.size(1) > self.max_seq_length:\n",
        "                    current_seq = current_seq[:, -self.max_seq_length:]\n",
        "\n",
        "                # Get model predictions\n",
        "                logits = self(current_seq)\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Sample next token\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Append next token to sequence\n",
        "                current_seq = torch.cat([current_seq, next_token], dim=1)\n",
        "\n",
        "                # Check if we've generated an EOS token\n",
        "                if next_token.item() == self.vocab_size - 1:  # Assuming EOS is the last token\n",
        "                    break\n",
        "\n",
        "        return current_seq"
      ],
      "metadata": {
        "id": "hs7mWseS3hWV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Displaying the Decoder-only Transformer Architecture"
      ],
      "metadata": {
        "id": "YyhgXe9X7ytF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "# Initialize the model with some example parameters\n",
        "vocab_size = 10000\n",
        "d_model = 512\n",
        "nhead = 2\n",
        "num_layers = 1\n",
        "d_ff = 2048\n",
        "max_seq_length = 1024\n",
        "dropout = 0.1\n",
        "\n",
        "# Define your model\n",
        "model = DecoderOnlyTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, input_size=(1, max_seq_length), dtypes=[torch.int64])"
      ],
      "metadata": {
        "id": "49nYvRll78Ug",
        "outputId": "c50f1176-8814-4f9b-acea-cb6c4ac950e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "DecoderOnlyTransformer                        [1, 1024, 10000]          --\n",
              "├─Embedding: 1-1                              [1, 1024, 512]            5,120,000\n",
              "├─PositionalEncoding: 1-2                     [1, 1024, 512]            --\n",
              "├─ModuleList: 1-3                             --                        --\n",
              "│    └─DecoderLayer: 2-1                      [1, 1024, 512]            --\n",
              "│    │    └─LayerNorm: 3-1                    [1, 1024, 512]            1,024\n",
              "│    │    └─MaskedAttention: 3-2              [1, 1024, 512]            1,050,624\n",
              "│    │    └─LayerNorm: 3-3                    [1, 1024, 512]            (recursive)\n",
              "│    │    └─FeedForward: 3-4                  [1, 1024, 512]            2,099,712\n",
              "├─LayerNorm: 1-4                              [1, 1024, 512]            1,024\n",
              "├─Linear: 1-5                                 [1, 1024, 10000]          5,130,000\n",
              "===============================================================================================\n",
              "Total params: 13,402,384\n",
              "Trainable params: 13,402,384\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 12.35\n",
              "===============================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 119.67\n",
              "Params size (MB): 49.41\n",
              "Estimated Total Size (MB): 169.08\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Training the Decoder-only Transformer"
      ],
      "metadata": {
        "id": "qJMRxOoSB0Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tiny_shakespeare dataset\n",
        "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
        "# Load the tiny_shakespeare dataset\n",
        "# dataset = load_dataset(\"lyimo/shakespear\", split=\"train\")\n",
        "\n",
        "# Extract the text from the dataset\n",
        "texts = dataset[\"text\"]\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 256\n",
        "nhead = 2\n",
        "num_layers = 2\n",
        "d_ff = 256\n",
        "max_seq_length = 128\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001\n",
        "dropout = 0.2\n",
        "\n",
        "# Tokenize and prepare data\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.fit(texts)\n",
        "vocab_size = len(tokenizer.word_to_idx)\n",
        "\n",
        "dataset = TextDataset(texts, tokenizer, max_seq_length)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model and move to device\n",
        "model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_idx[\"<PAD>\"])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_seq, _, _ = batch  # Unpack batch\n",
        "        input_seq = input_seq.squeeze(1).to(device)  # Move input to device and remove extra dimension\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(input_seq)\n",
        "\n",
        "        # Reshape output tensor\n",
        "        output = output[:, :-1, :].contiguous().view(-1, output.size(-1))  # Shift predictions to the left\n",
        "\n",
        "        # Shift targets to the right (original targets)\n",
        "        target_seq = input_seq[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target_seq)\n",
        "\n",
        "        # Debugging prints\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "          # Debugging prints\n",
        "          print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}\")\n",
        "          print(f\"Input sequence shape: {input_seq.shape}\")\n",
        "          print(f\"Input sequence: {input_seq.unsqueeze(1)}\")\n",
        "          print(f\"Output shape before reshape: {output.shape}\")\n",
        "          print(f\"Output shape after reshape: {output.shape}\")\n",
        "          print(f\"Target sequence shape: {target_seq.shape}\")\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "gSUt8j7yBrcP",
        "outputId": "80db0939-f757-4bfc-d133-a4cd0f3deac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 23845\n",
            "Loss: 10.255754470825195\n",
            "Epoch: 1, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,  7091,   635,  ...,  3661,  4681, 13579]],\n",
            "\n",
            "        [[    2,   964,   144,  ...,  4844,    44,   596]],\n",
            "\n",
            "        [[    2, 18162,   673,  ...,  8186, 18178,   142]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,   647,  5644,  ...,  8880,   142,  4427]],\n",
            "\n",
            "        [[    2,    94,   367,  ...,  5956,   379,  5734]],\n",
            "\n",
            "        [[    2,   501,    59,  ..., 14406,  1002,   104]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 10.256316184997559\n",
            "Loss: 10.257160186767578\n",
            "Loss: 10.2410306930542\n",
            "Loss: 10.239019393920898\n",
            "Loss: 10.232219696044922\n",
            "Loss: 10.24264144897461\n",
            "Loss: 10.228175163269043\n",
            "Loss: 10.22819709777832\n",
            "Loss: 10.211610794067383\n",
            "Loss: 10.207146644592285\n",
            "Loss: 10.212985038757324\n",
            "Loss: 10.213184356689453\n",
            "Loss: 10.207633972167969\n",
            "Loss: 10.205053329467773\n",
            "Loss: 10.198847770690918\n",
            "Loss: 10.198590278625488\n",
            "Loss: 10.187643051147461\n",
            "Loss: 10.187417030334473\n",
            "Loss: 10.184471130371094\n",
            "Loss: 10.185612678527832\n",
            "Loss: 10.167372703552246\n",
            "Loss: 10.177888870239258\n",
            "Loss: 10.16525936126709\n",
            "Loss: 10.169323921203613\n",
            "Loss: 10.164294242858887\n",
            "Loss: 10.155640602111816\n",
            "Loss: 10.144556999206543\n",
            "Loss: 10.145278930664062\n",
            "Loss: 10.148144721984863\n",
            "Loss: 10.141488075256348\n",
            "Loss: 10.143505096435547\n",
            "Loss: 10.116886138916016\n",
            "Loss: 10.121707916259766\n",
            "Loss: 10.126063346862793\n",
            "Loss: 10.100187301635742\n",
            "Loss: 10.091644287109375\n",
            "Loss: 10.089315414428711\n",
            "Epoch 1/10, Loss: 10.1829\n",
            "Loss: 10.05774211883545\n",
            "Epoch: 2, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,    49,  4989,  ...,    49,  5618,   120]],\n",
            "\n",
            "        [[    2,   410,  1923,  ...,   285, 22590,    82]],\n",
            "\n",
            "        [[    2, 21569,    61,  ..., 21473,    32, 21581]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,   595,   122,  ...,  9144,   369,   135]],\n",
            "\n",
            "        [[    2,  2083,   221,  ...,   235,  6589,  6619]],\n",
            "\n",
            "        [[    2,    35,  2469,  ...,   172,   285,  2501]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 10.058581352233887\n",
            "Loss: 10.048999786376953\n",
            "Loss: 10.03701400756836\n",
            "Loss: 10.03036117553711\n",
            "Loss: 10.014993667602539\n",
            "Loss: 10.010910034179688\n",
            "Loss: 10.003185272216797\n",
            "Loss: 9.990053176879883\n",
            "Loss: 9.981467247009277\n",
            "Loss: 9.971220970153809\n",
            "Loss: 9.96152400970459\n",
            "Loss: 9.944417953491211\n",
            "Loss: 9.933893203735352\n",
            "Loss: 9.924528121948242\n",
            "Loss: 9.888384819030762\n",
            "Loss: 9.899237632751465\n",
            "Loss: 9.860406875610352\n",
            "Loss: 9.842180252075195\n",
            "Loss: 9.853081703186035\n",
            "Loss: 9.823554039001465\n",
            "Loss: 9.793174743652344\n",
            "Loss: 9.789508819580078\n",
            "Loss: 9.773993492126465\n",
            "Loss: 9.74793815612793\n",
            "Loss: 9.710501670837402\n",
            "Loss: 9.706560134887695\n",
            "Loss: 9.673028945922852\n",
            "Loss: 9.662980079650879\n",
            "Loss: 9.633784294128418\n",
            "Loss: 9.591327667236328\n",
            "Loss: 9.597046852111816\n",
            "Loss: 9.553796768188477\n",
            "Loss: 9.512223243713379\n",
            "Loss: 9.500768661499023\n",
            "Loss: 9.478023529052734\n",
            "Loss: 9.44528865814209\n",
            "Loss: 9.420110702514648\n",
            "Epoch 2/10, Loss: 9.8086\n",
            "Loss: 9.375290870666504\n",
            "Epoch: 3, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,  1057,   122,  ...,  9241,  1518,   104]],\n",
            "\n",
            "        [[    2, 12302,    44,  ...,  6068,  1290,  4279]],\n",
            "\n",
            "        [[    2,  6684,  6685,  ...,   120,   629,  6723]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,   369,   501,  ...,   267,   142,   688]],\n",
            "\n",
            "        [[    2,   115, 16861,  ...,  7170,   179,    32]],\n",
            "\n",
            "        [[    2,   699,   100,  ...,  2088,   164,    61]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 9.346158981323242\n",
            "Loss: 9.31689739227295\n",
            "Loss: 9.288908958435059\n",
            "Loss: 9.234600067138672\n",
            "Loss: 9.227468490600586\n",
            "Loss: 9.184928894042969\n",
            "Loss: 9.148279190063477\n",
            "Loss: 9.08676528930664\n",
            "Loss: 9.088892936706543\n",
            "Loss: 9.025107383728027\n",
            "Loss: 9.0070161819458\n",
            "Loss: 8.980097770690918\n",
            "Loss: 8.949970245361328\n",
            "Loss: 8.91254711151123\n",
            "Loss: 8.854451179504395\n",
            "Loss: 8.826665878295898\n",
            "Loss: 8.805636405944824\n",
            "Loss: 8.7465181350708\n",
            "Loss: 8.7322998046875\n",
            "Loss: 8.71127700805664\n",
            "Loss: 8.662763595581055\n",
            "Loss: 8.627700805664062\n",
            "Loss: 8.6301908493042\n",
            "Loss: 8.533323287963867\n",
            "Loss: 8.522053718566895\n",
            "Loss: 8.491823196411133\n",
            "Loss: 8.442548751831055\n",
            "Loss: 8.465299606323242\n",
            "Loss: 8.405591011047363\n",
            "Loss: 8.361998558044434\n",
            "Loss: 8.352705001831055\n",
            "Loss: 8.344642639160156\n",
            "Loss: 8.267833709716797\n",
            "Loss: 8.290093421936035\n",
            "Loss: 8.224198341369629\n",
            "Loss: 8.258870124816895\n",
            "Loss: 8.138127326965332\n",
            "Epoch 3/10, Loss: 8.7597\n",
            "Loss: 8.169597625732422\n",
            "Epoch: 4, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,  8112,   412,  ...,    98,    49,  2247]],\n",
            "\n",
            "        [[    2,   122,  1257,  ..., 10990,  1224,   135]],\n",
            "\n",
            "        [[    2,   185, 10305,  ...,  1296,    98,   122]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,   122,  5988,  ...,  6016,    85,   543]],\n",
            "\n",
            "        [[    2,   621,     9,  ...,    35, 13486,  2347]],\n",
            "\n",
            "        [[    2,  3864,  1089,  ...,    35,  3898,  3899]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 8.112305641174316\n",
            "Loss: 8.09900188446045\n",
            "Loss: 8.0775728225708\n",
            "Loss: 8.060303688049316\n",
            "Loss: 8.033563613891602\n",
            "Loss: 8.012234687805176\n",
            "Loss: 8.025842666625977\n",
            "Loss: 7.958576679229736\n",
            "Loss: 7.927702903747559\n",
            "Loss: 7.918918609619141\n",
            "Loss: 7.928677082061768\n",
            "Loss: 7.885840892791748\n",
            "Loss: 7.901925563812256\n",
            "Loss: 7.817755222320557\n",
            "Loss: 7.841269016265869\n",
            "Loss: 7.835002422332764\n",
            "Loss: 7.854440689086914\n",
            "Loss: 7.805274963378906\n",
            "Loss: 7.763054370880127\n",
            "Loss: 7.764680862426758\n",
            "Loss: 7.772843360900879\n",
            "Loss: 7.756424427032471\n",
            "Loss: 7.784286975860596\n",
            "Loss: 7.720322608947754\n",
            "Loss: 7.699152946472168\n",
            "Loss: 7.6910576820373535\n",
            "Loss: 7.64241361618042\n",
            "Loss: 7.661470413208008\n",
            "Loss: 7.684629440307617\n",
            "Loss: 7.670234203338623\n",
            "Loss: 7.645886421203613\n",
            "Loss: 7.6595025062561035\n",
            "Loss: 7.674023628234863\n",
            "Loss: 7.637602806091309\n",
            "Loss: 7.655361175537109\n",
            "Loss: 7.626909255981445\n",
            "Loss: 7.5658955574035645\n",
            "Epoch 4/10, Loss: 7.8248\n",
            "Loss: 7.634414196014404\n",
            "Epoch: 5, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,    32,    85,  ...,  5167,   147,  3729]],\n",
            "\n",
            "        [[    2,  1398,   124,  ...,   595,  8160,   264]],\n",
            "\n",
            "        [[    2,    46,   122,  ..., 12719,   926,  3421]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,  1717,    98,  ...,   122,    88,  5286]],\n",
            "\n",
            "        [[    2,  1533,  8719,  ...,     9,  5270,   122]],\n",
            "\n",
            "        [[    2,   102,    98,  ...,  2432,  2907,   235]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.598653793334961\n",
            "Loss: 7.598208427429199\n",
            "Loss: 7.560103416442871\n",
            "Loss: 7.575143337249756\n",
            "Loss: 7.532095432281494\n",
            "Loss: 7.596346855163574\n",
            "Loss: 7.549859523773193\n",
            "Loss: 7.609068393707275\n",
            "Loss: 7.579855442047119\n",
            "Loss: 7.630288124084473\n",
            "Loss: 7.541948318481445\n",
            "Loss: 7.506217002868652\n",
            "Loss: 7.478592395782471\n",
            "Loss: 7.572175979614258\n",
            "Loss: 7.579098224639893\n",
            "Loss: 7.519004821777344\n",
            "Loss: 7.506300449371338\n",
            "Loss: 7.604742527008057\n",
            "Loss: 7.547571659088135\n",
            "Loss: 7.555083274841309\n",
            "Loss: 7.56962251663208\n",
            "Loss: 7.543537616729736\n",
            "Loss: 7.5407185554504395\n",
            "Loss: 7.521007061004639\n",
            "Loss: 7.495777130126953\n",
            "Loss: 7.562196731567383\n",
            "Loss: 7.510167598724365\n",
            "Loss: 7.492552280426025\n",
            "Loss: 7.493614196777344\n",
            "Loss: 7.503611087799072\n",
            "Loss: 7.5388875007629395\n",
            "Loss: 7.5118632316589355\n",
            "Loss: 7.541889667510986\n",
            "Loss: 7.517987251281738\n",
            "Loss: 7.468125343322754\n",
            "Loss: 7.539727687835693\n",
            "Loss: 7.5503411293029785\n",
            "Epoch 5/10, Loss: 7.5467\n",
            "Loss: 7.542264938354492\n",
            "Epoch: 6, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,   147,  3543,  ...,  4629,   284,   179]],\n",
            "\n",
            "        [[    2,    53,   453,  ...,   239,  2837,    53]],\n",
            "\n",
            "        [[    2,  7550,   147,  ...,  7117,   102,  7836]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,    53,  2410,  ...,  1638,  1978,  1138]],\n",
            "\n",
            "        [[    2,  1909, 18777,  ...,   861,   122,   151]],\n",
            "\n",
            "        [[    2,    35,  5333,  ...,  5232,   640,    53]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.492741584777832\n",
            "Loss: 7.509838581085205\n",
            "Loss: 7.47440242767334\n",
            "Loss: 7.501360893249512\n",
            "Loss: 7.499392509460449\n",
            "Loss: 7.5260114669799805\n",
            "Loss: 7.494965076446533\n",
            "Loss: 7.503133296966553\n",
            "Loss: 7.512931823730469\n",
            "Loss: 7.4683990478515625\n",
            "Loss: 7.475309371948242\n",
            "Loss: 7.502170562744141\n",
            "Loss: 7.4760332107543945\n",
            "Loss: 7.493992328643799\n",
            "Loss: 7.45378303527832\n",
            "Loss: 7.4669294357299805\n",
            "Loss: 7.4439544677734375\n",
            "Loss: 7.46720027923584\n",
            "Loss: 7.486845970153809\n",
            "Loss: 7.476139068603516\n",
            "Loss: 7.469913005828857\n",
            "Loss: 7.4901275634765625\n",
            "Loss: 7.45828104019165\n",
            "Loss: 7.472944736480713\n",
            "Loss: 7.493595600128174\n",
            "Loss: 7.501047134399414\n",
            "Loss: 7.525229454040527\n",
            "Loss: 7.447473526000977\n",
            "Loss: 7.455968379974365\n",
            "Loss: 7.477691173553467\n",
            "Loss: 7.459994316101074\n",
            "Loss: 7.547706127166748\n",
            "Loss: 7.45605993270874\n",
            "Loss: 7.549522876739502\n",
            "Loss: 7.494457721710205\n",
            "Loss: 7.501151084899902\n",
            "Loss: 7.447311878204346\n",
            "Epoch 6/10, Loss: 7.4873\n",
            "Loss: 7.422071933746338\n",
            "Epoch: 7, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,   124,   142,  ...,   122, 18124,    28]],\n",
            "\n",
            "        [[    2,   911,    53,  ...,   124,    35,  3784]],\n",
            "\n",
            "        [[    2,    32,  5053,  ...,    44,  1135,  2329]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,   395,   966,  ...,    35,    71, 11896]],\n",
            "\n",
            "        [[    2,    68,  1823,  ...,  3848,  6843,   619]],\n",
            "\n",
            "        [[    2,   291, 10969,  ..., 21091,   122,   300]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.4835638999938965\n",
            "Loss: 7.452077865600586\n",
            "Loss: 7.438135623931885\n",
            "Loss: 7.469388008117676\n",
            "Loss: 7.486128807067871\n",
            "Loss: 7.40745210647583\n",
            "Loss: 7.427803993225098\n",
            "Loss: 7.462096214294434\n",
            "Loss: 7.5204572677612305\n",
            "Loss: 7.4147772789001465\n",
            "Loss: 7.447430610656738\n",
            "Loss: 7.441377639770508\n",
            "Loss: 7.425530910491943\n",
            "Loss: 7.439289093017578\n",
            "Loss: 7.54311990737915\n",
            "Loss: 7.512543201446533\n",
            "Loss: 7.454799652099609\n",
            "Loss: 7.4603047370910645\n",
            "Loss: 7.483011245727539\n",
            "Loss: 7.450984001159668\n",
            "Loss: 7.429054260253906\n",
            "Loss: 7.451001167297363\n",
            "Loss: 7.402708530426025\n",
            "Loss: 7.434037208557129\n",
            "Loss: 7.433046340942383\n",
            "Loss: 7.412294864654541\n",
            "Loss: 7.423313617706299\n",
            "Loss: 7.435471534729004\n",
            "Loss: 7.484220027923584\n",
            "Loss: 7.355982780456543\n",
            "Loss: 7.420058250427246\n",
            "Loss: 7.41873025894165\n",
            "Loss: 7.447849273681641\n",
            "Loss: 7.402385711669922\n",
            "Loss: 7.463097095489502\n",
            "Loss: 7.461702346801758\n",
            "Loss: 7.280213832855225\n",
            "Epoch 7/10, Loss: 7.4420\n",
            "Loss: 7.399221420288086\n",
            "Epoch: 8, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,  1283,  7085,  ...,   635,  7106,  6613]],\n",
            "\n",
            "        [[    2,   108,  6497,  ...,   989,    35,   404]],\n",
            "\n",
            "        [[    2,    16,   251,  ...,   124,    53,  4965]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2, 10607,  1765,  ...,  7117,   115,   285]],\n",
            "\n",
            "        [[    2, 21371,   102,  ...,   582,   122,   543]],\n",
            "\n",
            "        [[    2,   104,   122,  ...,   456,    44, 17226]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.373025417327881\n",
            "Loss: 7.450195789337158\n",
            "Loss: 7.437078952789307\n",
            "Loss: 7.417957782745361\n",
            "Loss: 7.408487319946289\n",
            "Loss: 7.405189037322998\n",
            "Loss: 7.420319557189941\n",
            "Loss: 7.367468357086182\n",
            "Loss: 7.415953636169434\n",
            "Loss: 7.384800434112549\n",
            "Loss: 7.3880157470703125\n",
            "Loss: 7.419422626495361\n",
            "Loss: 7.442048072814941\n",
            "Loss: 7.3940653800964355\n",
            "Loss: 7.39585018157959\n",
            "Loss: 7.4784836769104\n",
            "Loss: 7.394227504730225\n",
            "Loss: 7.4232330322265625\n",
            "Loss: 7.38326358795166\n",
            "Loss: 7.481940269470215\n",
            "Loss: 7.409613132476807\n",
            "Loss: 7.38280725479126\n",
            "Loss: 7.387669086456299\n",
            "Loss: 7.4222331047058105\n",
            "Loss: 7.342639923095703\n",
            "Loss: 7.359399795532227\n",
            "Loss: 7.464298725128174\n",
            "Loss: 7.390978813171387\n",
            "Loss: 7.359142780303955\n",
            "Loss: 7.445589065551758\n",
            "Loss: 7.3738203048706055\n",
            "Loss: 7.4486188888549805\n",
            "Loss: 7.390759468078613\n",
            "Loss: 7.391644477844238\n",
            "Loss: 7.387009620666504\n",
            "Loss: 7.343278408050537\n",
            "Loss: 7.323497295379639\n",
            "Epoch 8/10, Loss: 7.4027\n",
            "Loss: 7.399677753448486\n",
            "Epoch: 9, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2,    35,  7259,  ...,  4519,    12,    21]],\n",
            "\n",
            "        [[    2,  3406,  3022,  ...,   235,   658,    44]],\n",
            "\n",
            "        [[    2,   785,  6413,  ..., 14146, 15069,   966]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,  1306,   185,  ...,  1809,  4426,   635]],\n",
            "\n",
            "        [[    2,    53,  1109,  ...,  5868,    44,  2738]],\n",
            "\n",
            "        [[    2,   206, 11645,  ...,   675,    78,  1482]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.392134189605713\n",
            "Loss: 7.388767719268799\n",
            "Loss: 7.408994197845459\n",
            "Loss: 7.354116439819336\n",
            "Loss: 7.369341850280762\n",
            "Loss: 7.388487815856934\n",
            "Loss: 7.380352973937988\n",
            "Loss: 7.344897270202637\n",
            "Loss: 7.406322956085205\n",
            "Loss: 7.361561298370361\n",
            "Loss: 7.335497856140137\n",
            "Loss: 7.4006452560424805\n",
            "Loss: 7.366282939910889\n",
            "Loss: 7.3597025871276855\n",
            "Loss: 7.374234676361084\n",
            "Loss: 7.389711380004883\n",
            "Loss: 7.364117622375488\n",
            "Loss: 7.294205665588379\n",
            "Loss: 7.357673645019531\n",
            "Loss: 7.376742362976074\n",
            "Loss: 7.3073930740356445\n",
            "Loss: 7.315725326538086\n",
            "Loss: 7.3951311111450195\n",
            "Loss: 7.386874198913574\n",
            "Loss: 7.379457950592041\n",
            "Loss: 7.287999153137207\n",
            "Loss: 7.340630531311035\n",
            "Loss: 7.35132360458374\n",
            "Loss: 7.297338008880615\n",
            "Loss: 7.397643566131592\n",
            "Loss: 7.36543607711792\n",
            "Loss: 7.356167316436768\n",
            "Loss: 7.373354911804199\n",
            "Loss: 7.350930690765381\n",
            "Loss: 7.355933666229248\n",
            "Loss: 7.331043243408203\n",
            "Loss: 7.2013020515441895\n",
            "Epoch 9/10, Loss: 7.3581\n",
            "Loss: 7.355997562408447\n",
            "Epoch: 10, Batch: 1\n",
            "Input sequence shape: torch.Size([64, 128])\n",
            "Input sequence: tensor([[[    2, 15455,    28,  ...,    82,   368,    61]],\n",
            "\n",
            "        [[    2, 13578,  1498,  ...,   124,  1482,  7164]],\n",
            "\n",
            "        [[    2,    53,   714,  ...,   264,     7,    21]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,  3556, 15367,  ...,   685,  6695, 17713]],\n",
            "\n",
            "        [[    2, 14152,  2369,  ...,   235,   906,  1788]],\n",
            "\n",
            "        [[    2,   704,   895,  ...,   977,   330,     7]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([8128, 23845])\n",
            "Output shape after reshape: torch.Size([8128, 23845])\n",
            "Target sequence shape: torch.Size([8128])\n",
            "Loss: 7.315639495849609\n",
            "Loss: 7.334701061248779\n",
            "Loss: 7.336318016052246\n",
            "Loss: 7.335424900054932\n",
            "Loss: 7.319916248321533\n",
            "Loss: 7.354195594787598\n",
            "Loss: 7.332363128662109\n",
            "Loss: 7.275672912597656\n",
            "Loss: 7.2718119621276855\n",
            "Loss: 7.31134557723999\n",
            "Loss: 7.318399906158447\n",
            "Loss: 7.313045024871826\n",
            "Loss: 7.297952175140381\n",
            "Loss: 7.346749305725098\n",
            "Loss: 7.349934101104736\n",
            "Loss: 7.3151164054870605\n",
            "Loss: 7.373406410217285\n",
            "Loss: 7.324233055114746\n",
            "Loss: 7.275195121765137\n",
            "Loss: 7.336575508117676\n",
            "Loss: 7.339095115661621\n",
            "Loss: 7.335159778594971\n",
            "Loss: 7.304776191711426\n",
            "Loss: 7.28388786315918\n",
            "Loss: 7.322508811950684\n",
            "Loss: 7.2693986892700195\n",
            "Loss: 7.332945823669434\n",
            "Loss: 7.350222587585449\n",
            "Loss: 7.3550896644592285\n",
            "Loss: 7.277447700500488\n",
            "Loss: 7.270828723907471\n",
            "Loss: 7.36733341217041\n",
            "Loss: 7.295757293701172\n",
            "Loss: 7.275390148162842\n",
            "Loss: 7.324302673339844\n",
            "Loss: 7.285636901855469\n",
            "Loss: 7.0496039390563965\n",
            "Epoch 10/10, Loss: 7.3114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Decoder-only Transformer"
      ],
      "metadata": {
        "id": "XEkV3lWgCsHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Better three hours too soon than\", \" I believe I can \", \"My words fly up, my\", \"Brevity is \", \"Love looks not with the eyes, but\", \"To be or \"]\n",
        "\n",
        "for quote in texts:\n",
        "  start_tokens = torch.tensor(tokenizer.encode(quote)).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "  generated_tokens = model.generate(start_tokens, max_length=20, temperature=.9)\n",
        "  generated_text = tokenizer.decode(generated_tokens.squeeze().tolist())\n",
        "\n",
        "  print(generated_text)"
      ],
      "metadata": {
        "id": "ioC249m3-X2H",
        "outputId": "d890315d-25c7-4017-a707-11780307eed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Better three hours too soon than thou and of that I the Blood, gain; therefore in THOMAS you'ld than you\n",
            "I believe I can and and pray LEONTES: thy Dazzle seeming in stand The not Have Of I plainly the\n",
            "My words fly up, my suppose revenue of and granted I be have in either thee? this Do me. RICHARD\n",
            "<UNK> is Be will discretion, please CLEOMENES: her deceived; match, pray, the cries: time her for private that Of An\n",
            "Love looks not with the eyes, but had and That my she his My his beasts the troubler live And\n",
            "To be or so state months turn as it the bade lord, ROMEO: I good: hath thy part, art Clown:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________"
      ],
      "metadata": {
        "id": "6_9fxH9cKaGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder-only with MoE instead of FFN\n",
        "\n",
        "In the Sparse Mixture of Experts (MoE) architecture, the self-attention mechanism within each transformer block stays the same.\n",
        "\n",
        "However, a key modification is made to the structure **of each block**: the standard **feed-forward neural network** is replaced with **multiple sparsely activated feed-forward networks, known as experts.**\n",
        "\n",
        "\"Sparse activation\" means that each token in the sequence is routed to only a small number of these experts—usually one or two—out of the entire pool.\n",
        "\n",
        "\n",
        "\n",
        "**✨ Additional Resources:**\n",
        "\n",
        "*   makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch [Link-huggingface](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LYtAnvTb83kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________"
      ],
      "metadata": {
        "id": "Um0oVSr_KdMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Expert Layer"
      ],
      "metadata": {
        "id": "uDOiZz6LKT69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "    \"\"\" An MLP with a single hidden layer and ReLU activation, serving as an expert in a Mixture of Experts. \"\"\"\n",
        "    def __init__(self, n_embd: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "\n",
        "        #######################Code Here###############################\n",
        "\n",
        "        # Define a Linear, ReLU, Linaer and Dropout, Linear should be (n_embd, 4 * n_embd)\n",
        "\n",
        "        ###############################################################\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "\n",
        "      )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "S9PuG3xQKZcp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Gating in MoE Architectures\n",
        "\n",
        "Types of gating in Mixture of Experts (MoE) systems include Top-k gating, Noisy Top-k gating (as implemented here), and other variants like Hierarchical gating or Soft gating.\n",
        "\n",
        "Gating is essential in MoE systems because it determines which experts to use for each input, allowing the model to specialize different experts for different types of inputs or tasks.\n",
        "\n",
        "Specifically, Noisy Top-k gating adds controlled randomness to the expert selection process, which can help balance expert utilization and potentially improve model performance by introducing exploration in the routing mechanism."
      ],
      "metadata": {
        "id": "KhIocpeCKevX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k_moe):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        # Store the top_k_moe parameter which specifies the number of top experts to select\n",
        "        self.top_k_moe = top_k_moe\n",
        "        # Linear layer to compute logits for routing\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        # Linear layer to compute noise logits for added noise\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # Compute the logits for routing to experts\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        # Compute the noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "        # Generate noise with standard deviation determined by softplus of noise logits\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        # Add noise to the original logits to get noisy logits\n",
        "        noisy_logits = logits + noise\n",
        "        # Select the top k logits and their indices from the noisy logits\n",
        "        top_k_moe_logits, indices = noisy_logits.topk(self.top_k_moe, dim=-1)\n",
        "        # Create a tensor full of -inf values\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        # Scatter the top k logits into the zeros tensor to create a sparse logits tensor\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_moe_logits)\n",
        "        # Apply softmax to the sparse logits to get the final router output\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        # Return the router output and the indices of the selected experts\n",
        "        return router_output, indices"
      ],
      "metadata": {
        "id": "N1z8IuwGKjV3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Sparse MoE Layer"
      ],
      "metadata": {
        "id": "OXoO2q9IKlmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k_moe):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        # Initialize the NoisyTopkRouter to determine which experts to activate\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k_moe)\n",
        "        # Create a list of expert networks, each being a feed-forward network\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        # Store the number of top experts to activate\n",
        "        self.top_k_moe = top_k_moe\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the gating output and indices from the router\n",
        "        gating_output, indices = self.router(x)\n",
        "        # Initialize the final output tensor with zeros, having the same shape as x\n",
        "        final_output = torch.zeros_like(x)\n",
        "        # Flatten the input tensor to simplify processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        # Flatten the gating output tensor to align with the flattened input\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Iterate over each expert\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask to identify where the current expert is used\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            # Flatten the expert mask to match the flattened input\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():  # Check if there are any positions using the current expert\n",
        "                # Extract the inputs for the current expert based on the mask\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                # Get the output from the current expert\n",
        "                expert_output = expert(expert_input)\n",
        "                # Get the gating scores for the current expert\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                # Compute the weighted output based on the gating scores\n",
        "                weighted_output = expert_output * gating_scores\n",
        "                # Add the weighted output to the final output tensor\n",
        "                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n",
        "\n",
        "        # Return the final output tensor which combines the results from all activated experts\n",
        "        return final_output\n"
      ],
      "metadata": {
        "id": "N6kQTACyKvhF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Decoder with Sparse MoE\n",
        "\n",
        "![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n",
        "\n"
      ],
      "metadata": {
        "id": "pru3rAF3Kwg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayerMoE(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff, num_experts, top_k_moe, dropout=0.1):\n",
        "        super().__init__()\n",
        "        #######################Code Here###############################\n",
        "\n",
        "        # Create the Decoder architecture as before, but now add the MoE block instead of the FFN\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.masked_attention = MaskedAttention(d_model, nhead, dropout)\n",
        "        self.MOE_block = SparseMoE(d_model, num_experts, top_k_moe)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        normed_x = self.norm1(x)\n",
        "        attn_output, _ = self.masked_attention(normed_x) # _ because we returned also the mask in the previous demonstration\n",
        "        # Feed-Forward Network\n",
        "        normed_attn = self.norm1(attn_output)\n",
        "        moe_output = self.MOE_block(normed_attn)\n",
        "\n",
        "        return x + moe_output\n",
        "\n",
        "\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ],
      "metadata": {
        "id": "t38S7EsDK3_r"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Showcase how Sparse MoE handles its inputs\n",
        "\n",
        "-  Experiment: Change the number of experts in the SparseMoE_example model.\n",
        "  -  Observation: Observe how increasing or decreasing the number of experts affects the routing, gating outputs, and final output.\n",
        "\n",
        "- Experiment: Adjust the top_k_moe parameter to select more or fewer top experts.\n",
        "  - Observation: See how the number of experts activated for each token changes and how it impacts the final output.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGu0o0A00f3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SparseMoE_example(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k_moe):\n",
        "        super(SparseMoE_example, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k_moe)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k_moe = top_k_moe\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def forward_debug_example(self, x):\n",
        "        # Forward pass with debug prints\n",
        "        gating_output, indices = self.router(x)\n",
        "        print(\"Gating Output Shape:\", gating_output.shape)\n",
        "        print(\"Gating Output:\", gating_output)\n",
        "        print(\"Expert Indices Shape:\", indices.shape)\n",
        "        print(\"Expert Indices:\", indices)\n",
        "\n",
        "        print(\"Input Shape:\", x.shape)\n",
        "        print(\"Input:\", x)\n",
        "\n",
        "        final_output = torch.zeros_like(x)\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            print(\"\\n\" + \"-\"*50)\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                print(f\"Expert {i} Input Shape:\", expert_input.shape)\n",
        "                print(f\"Expert {i} Input:\", expert_input)\n",
        "\n",
        "                expert_output = expert(expert_input)\n",
        "                print(f\"Expert {i} Output Shape:\", expert_output.shape)\n",
        "                print(f\"Expert {i} Output:\", expert_output)\n",
        "\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                print(f\"Gating Scores for Expert {i}:\")\n",
        "                print(gating_scores.squeeze())\n",
        "\n",
        "                print(f\"Weighted Output for Expert {i}:\")\n",
        "                weighted_output = expert_output * gating_scores\n",
        "                print(weighted_output)\n",
        "\n",
        "                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n",
        "                print(f\"Expert {i} final_output Shape:\", final_output[expert_mask].shape)\n",
        "                print(f\"Expert {i} final_output:\", final_output[expert_mask])\n",
        "\n",
        "        print(\"Final MoE Output Shape:\", final_output.shape)\n",
        "        print(\"Final MoE Output:\", final_output)\n",
        "        print(\"-\"*50)\n",
        "        return final_output\n",
        "\n",
        "\n",
        "# Example usage and debugging prints\n",
        "def test_sparse_moe():\n",
        "    # Parameters\n",
        "    batch_size = 2   #\n",
        "    seq_length = 1   # number of tokens, if 1 then it is easier to see which experts are activated and how each embedding is calculated\n",
        "    n_embed = 5\n",
        "    num_experts = 6  # Increased number of experts\n",
        "    top_k_moe = 2   # If you modify, more or less experts will be activated for each input token\n",
        "\n",
        "    # Random input tensor (simulating token embeddings)\n",
        "    random_input = torch.randn(batch_size, seq_length, n_embed)\n",
        "\n",
        "    # Initialize SparseMoE\n",
        "    sparse_moe = SparseMoE_example(n_embed, num_experts, top_k_moe)\n",
        "\n",
        "    # Forward pass with debugging example\n",
        "    final_output = sparse_moe.forward_debug_example(random_input)\n",
        "\n",
        "    print(\"\\nRandom Input Tensor:\")\n",
        "    print(random_input)\n",
        "    print(\"\\nFinal Output Tensor (after MoE processing):\")\n",
        "    print(final_output)\n",
        "\n",
        "# Run the test function\n",
        "test_sparse_moe()"
      ],
      "metadata": {
        "id": "9DBAPMat0oJh",
        "outputId": "19ad05e2-562d-4aa4-f763-3c7f6d96c83d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gating Output Shape: torch.Size([2, 1, 6])\n",
            "Gating Output: tensor([[[0.0000, 0.0000, 0.5771, 0.4229, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.6176, 0.3824, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Expert Indices Shape: torch.Size([2, 1, 2])\n",
            "Expert Indices: tensor([[[2, 3]],\n",
            "\n",
            "        [[0, 1]]])\n",
            "Input Shape: torch.Size([2, 1, 5])\n",
            "Input: tensor([[[-0.7378,  1.4550,  0.2411,  0.7767, -1.4454]],\n",
            "\n",
            "        [[-0.8564, -1.0963,  0.3714,  0.6694,  1.0280]]])\n",
            "\n",
            "--------------------------------------------------\n",
            "Expert 0 Input Shape: torch.Size([1, 5])\n",
            "Expert 0 Input: tensor([[-0.8564, -1.0963,  0.3714,  0.6694,  1.0280]])\n",
            "Expert 0 Output Shape: torch.Size([1, 5])\n",
            "Expert 0 Output: tensor([[-0.0333,  0.1922,  0.3914,  0.2914, -0.1683]], grad_fn=<MulBackward0>)\n",
            "Gating Scores for Expert 0:\n",
            "tensor(0.6176, grad_fn=<SqueezeBackward0>)\n",
            "Weighted Output for Expert 0:\n",
            "tensor([[-0.0206,  0.1187,  0.2417,  0.1800, -0.1040]], grad_fn=<MulBackward0>)\n",
            "Expert 0 final_output Shape: torch.Size([1, 5])\n",
            "Expert 0 final_output: tensor([[-0.0206,  0.1187,  0.2417,  0.1800, -0.1040]],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "\n",
            "--------------------------------------------------\n",
            "Expert 1 Input Shape: torch.Size([1, 5])\n",
            "Expert 1 Input: tensor([[-0.8564, -1.0963,  0.3714,  0.6694,  1.0280]])\n",
            "Expert 1 Output Shape: torch.Size([1, 5])\n",
            "Expert 1 Output: tensor([[-0.1403,  0.1189, -0.2311,  0.3700, -0.1003]], grad_fn=<MulBackward0>)\n",
            "Gating Scores for Expert 1:\n",
            "tensor(0.3824, grad_fn=<SqueezeBackward0>)\n",
            "Weighted Output for Expert 1:\n",
            "tensor([[-0.0537,  0.0455, -0.0884,  0.1415, -0.0383]], grad_fn=<MulBackward0>)\n",
            "Expert 1 final_output Shape: torch.Size([1, 5])\n",
            "Expert 1 final_output: tensor([[-0.0742,  0.1642,  0.1533,  0.3215, -0.1423]],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "\n",
            "--------------------------------------------------\n",
            "Expert 2 Input Shape: torch.Size([1, 5])\n",
            "Expert 2 Input: tensor([[-0.7378,  1.4550,  0.2411,  0.7767, -1.4454]])\n",
            "Expert 2 Output Shape: torch.Size([1, 5])\n",
            "Expert 2 Output: tensor([[ 0.0646, -0.1595,  0.3146, -0.0000,  0.2159]], grad_fn=<MulBackward0>)\n",
            "Gating Scores for Expert 2:\n",
            "tensor(0.5771, grad_fn=<SqueezeBackward0>)\n",
            "Weighted Output for Expert 2:\n",
            "tensor([[ 0.0373, -0.0921,  0.1815, -0.0000,  0.1246]], grad_fn=<MulBackward0>)\n",
            "Expert 2 final_output Shape: torch.Size([1, 5])\n",
            "Expert 2 final_output: tensor([[ 0.0373, -0.0921,  0.1815,  0.0000,  0.1246]],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "\n",
            "--------------------------------------------------\n",
            "Expert 3 Input Shape: torch.Size([1, 5])\n",
            "Expert 3 Input: tensor([[-0.7378,  1.4550,  0.2411,  0.7767, -1.4454]])\n",
            "Expert 3 Output Shape: torch.Size([1, 5])\n",
            "Expert 3 Output: tensor([[-0.2435, -0.6338,  0.0000, -0.0606, -0.5542]], grad_fn=<MulBackward0>)\n",
            "Gating Scores for Expert 3:\n",
            "tensor(0.4229, grad_fn=<SqueezeBackward0>)\n",
            "Weighted Output for Expert 3:\n",
            "tensor([[-0.1030, -0.2680,  0.0000, -0.0256, -0.2343]], grad_fn=<MulBackward0>)\n",
            "Expert 3 final_output Shape: torch.Size([1, 5])\n",
            "Expert 3 final_output: tensor([[-0.0657, -0.3601,  0.1815, -0.0256, -0.1098]],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--------------------------------------------------\n",
            "Final MoE Output Shape: torch.Size([2, 1, 5])\n",
            "Final MoE Output: tensor([[[-0.0657, -0.3601,  0.1815, -0.0256, -0.1098]],\n",
            "\n",
            "        [[-0.0742,  0.1642,  0.1533,  0.3215, -0.1423]]],\n",
            "       grad_fn=<IndexPutBackward0>)\n",
            "--------------------------------------------------\n",
            "\n",
            "Random Input Tensor:\n",
            "tensor([[[-0.7378,  1.4550,  0.2411,  0.7767, -1.4454]],\n",
            "\n",
            "        [[-0.8564, -1.0963,  0.3714,  0.6694,  1.0280]]])\n",
            "\n",
            "Final Output Tensor (after MoE processing):\n",
            "tensor([[[-0.0657, -0.3601,  0.1815, -0.0256, -0.1098]],\n",
            "\n",
            "        [[-0.0742,  0.1642,  0.1533,  0.3215, -0.1423]]],\n",
            "       grad_fn=<IndexPutBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Decoder-only Transformer with MoE"
      ],
      "metadata": {
        "id": "5BfeO9sAK5YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformerMoE(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout, num_experts, top_k_moe):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayerMoE(d_model, nhead, d_ff, num_experts, top_k_moe, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k_moe = top_k_moe\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.output(x)\n",
        "\n",
        "    def generate(self, start_tokens: torch.Tensor, max_length: int, temperature: float = 1.0) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        current_seq = start_tokens\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "          # Generate tokens until max_length is reached or end token is generated\n",
        "          for _ in range(max_length - start_tokens.size(1)):\n",
        "              # Ensure the sequence length does not exceed max_seq_length\n",
        "              if current_seq.size(1) > self.max_seq_length:\n",
        "                  current_seq = current_seq[:, -self.max_seq_length:]\n",
        "\n",
        "              # Get logits from the model\n",
        "              logits = self(current_seq)\n",
        "\n",
        "              # Extract logits for the next token and scale by temperature\n",
        "              next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "              # Compute probabilities using softmax\n",
        "              probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "              # Sample the next token from the probability distribution\n",
        "              next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "              # Append the next token to the current sequence\n",
        "              current_seq = torch.cat([current_seq, next_token], dim=1)\n",
        "\n",
        "              # Stop if the end token is generated (vocab_size - 1 assumed to be the end token)\n",
        "              if next_token.item() == self.vocab_size - 1:\n",
        "                  break\n",
        "\n",
        "        # Return the generated sequence\n",
        "        return current_seq"
      ],
      "metadata": {
        "id": "KvbjHbTCLEuO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Displaying the Decoder-only Transformer with MoE Architecture"
      ],
      "metadata": {
        "id": "gXwM_wnqLL4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model with some example parameters\n",
        "vocab_size = 10000\n",
        "d_model = 512\n",
        "nhead = 2\n",
        "num_layers = 1\n",
        "d_ff = 2048\n",
        "max_seq_length = 1024\n",
        "dropout = 0.1\n",
        "num_experts = 4\n",
        "top_k_moe = 2\n",
        "\n",
        "# Define your model\n",
        "model = DecoderOnlyTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "# Define your model\n",
        "model_moe = DecoderOnlyTransformerMoE(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout,\n",
        "    num_experts=num_experts,\n",
        "    top_k_moe=top_k_moe\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "print(50*\"-\")\n",
        "print(summary(model, input_size=(1, max_seq_length), dtypes=[torch.int64]))\n",
        "print(50*\"-\")\n",
        "print(summary(model_moe, input_size=(1, max_seq_length), dtypes=[torch.int64]))"
      ],
      "metadata": {
        "id": "HPDbqV4ULPmA",
        "outputId": "4b09f148-ad79-4538-b4c8-77b356e3856f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "DecoderOnlyTransformer                        [1, 1024, 10000]          --\n",
            "├─Embedding: 1-1                              [1, 1024, 512]            5,120,000\n",
            "├─PositionalEncoding: 1-2                     [1, 1024, 512]            --\n",
            "├─ModuleList: 1-3                             --                        --\n",
            "│    └─DecoderLayer: 2-1                      [1, 1024, 512]            --\n",
            "│    │    └─LayerNorm: 3-1                    [1, 1024, 512]            1,024\n",
            "│    │    └─MaskedAttention: 3-2              [1, 1024, 512]            1,050,624\n",
            "│    │    └─LayerNorm: 3-3                    [1, 1024, 512]            (recursive)\n",
            "│    │    └─FeedForward: 3-4                  [1, 1024, 512]            2,099,712\n",
            "├─LayerNorm: 1-4                              [1, 1024, 512]            1,024\n",
            "├─Linear: 1-5                                 [1, 1024, 10000]          5,130,000\n",
            "===============================================================================================\n",
            "Total params: 13,402,384\n",
            "Trainable params: 13,402,384\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 12.35\n",
            "===============================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 119.67\n",
            "Params size (MB): 49.41\n",
            "Estimated Total Size (MB): 169.08\n",
            "===============================================================================================\n",
            "--------------------------------------------------\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "DecoderOnlyTransformerMoE                          [1, 1024, 10000]          --\n",
            "├─Embedding: 1-1                                   [1, 1024, 512]            5,120,000\n",
            "├─PositionalEncoding: 1-2                          [1, 1024, 512]            --\n",
            "├─ModuleList: 1-3                                  --                        --\n",
            "│    └─DecoderLayerMoE: 2-1                        [1, 1024, 512]            --\n",
            "│    │    └─LayerNorm: 3-1                         [1, 1024, 512]            1,024\n",
            "│    │    └─MaskedAttention: 3-2                   [1, 1024, 512]            1,050,624\n",
            "│    │    └─LayerNorm: 3-3                         [1, 1024, 512]            (recursive)\n",
            "│    │    └─SparseMoE: 3-4                         [1, 1024, 512]            8,402,952\n",
            "├─LayerNorm: 1-4                                   [1, 1024, 512]            1,024\n",
            "├─Linear: 1-5                                      [1, 1024, 10000]          5,130,000\n",
            "====================================================================================================\n",
            "Total params: 19,705,624\n",
            "Trainable params: 19,705,624\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 4.31\n",
            "====================================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 140.71\n",
            "Params size (MB): 74.62\n",
            "Estimated Total Size (MB): 215.33\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_model_summary(model, input_size):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    dummy_input = torch.zeros(input_size, dtype=torch.int64).to(device)\n",
        "\n",
        "    def register_hook(module):\n",
        "        def hook(module, input, output):\n",
        "            class_name = module.__class__.__name__\n",
        "            module_idx = len(summary)\n",
        "            m_key = f\"{module_idx:03d} {class_name}\"\n",
        "            summary[m_key] = {}\n",
        "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
        "            if isinstance(output, torch.Tensor):\n",
        "                summary[m_key][\"output_shape\"] = list(output.size())\n",
        "            elif isinstance(output, (tuple, list)) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n",
        "                summary[m_key][\"output_shape\"] = [list(out.size()) for out in output]\n",
        "            else:\n",
        "                summary[m_key][\"output_shape\"] = \"multiple outputs\"\n",
        "            params = sum(p.numel() for p in module.parameters())\n",
        "            summary[m_key][\"num_params\"] = params\n",
        "\n",
        "        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n",
        "            hooks.append(module.register_forward_hook(hook))\n",
        "\n",
        "    summary = {}\n",
        "    hooks = []\n",
        "    model.apply(register_hook)\n",
        "    model(dummy_input)\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    print(\"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Input Shape\", \"Param #\"))\n",
        "    print(\"================================================================\")\n",
        "    total_params = 0\n",
        "    total_output = 0\n",
        "    for layer in summary:\n",
        "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
        "            layer,\n",
        "            str(summary[layer][\"input_shape\"]),\n",
        "            \"{0:,}\".format(summary[layer][\"num_params\"]),\n",
        "        )\n",
        "        total_params += summary[layer][\"num_params\"]\n",
        "        if isinstance(summary[layer][\"output_shape\"], list) and all(isinstance(i, int) for i in summary[layer][\"output_shape\"]):\n",
        "            total_output += np.prod(summary[layer][\"output_shape\"])\n",
        "        print(line_new)\n",
        "    print(\"================================================================\")\n",
        "    print(f\"Total params: {total_params:,}\")\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "\n",
        "vocab_size = 10000\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_layers = 1\n",
        "d_ff = 2048\n",
        "max_seq_length = 1024\n",
        "dropout = 0.1\n",
        "num_experts = 2\n",
        "top_k_moe = 1\n",
        "\n",
        "# Ensure you have the correct definitions for these classes\n",
        "# from your_model_definitions import DecoderOnlyTransformer, DecoderOnlyTransformerMoE\n",
        "\n",
        "model = DecoderOnlyTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "model_moe = DecoderOnlyTransformerMoE(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout,\n",
        "    num_experts=num_experts,\n",
        "    top_k_moe=top_k_moe\n",
        ")\n",
        "\n",
        "print(\"Summary for DecoderOnlyTransformer\")\n",
        "print_model_summary(model, (1, max_seq_length))\n",
        "\n",
        "print(\"\\nSummary for DecoderOnlyTransformerMoE\")\n",
        "print_model_summary(model_moe, (1, max_seq_length))\n"
      ],
      "metadata": {
        "id": "MGHFvdZwpdqT",
        "outputId": "1895f283-399b-4330-b56e-9d9e300fd3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for DecoderOnlyTransformer\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)                Input Shape         Param #\n",
            "================================================================\n",
            "       000 Embedding                  [1, 1024]       5,120,000\n",
            "001 PositionalEncoding             [1, 1024, 512]               0\n",
            "       002 LayerNorm             [1, 1024, 512]           1,024\n",
            "003 MultiheadAttention             [1, 1024, 512]       1,050,624\n",
            " 004 MaskedAttention             [1, 1024, 512]       1,050,624\n",
            "       005 LayerNorm             [1, 1024, 512]           1,024\n",
            "          006 Linear             [1, 1024, 512]       1,050,624\n",
            "            007 ReLU            [1, 1024, 2048]               0\n",
            "         008 Dropout            [1, 1024, 2048]               0\n",
            "          009 Linear            [1, 1024, 2048]       1,049,088\n",
            "     010 FeedForward             [1, 1024, 512]       2,099,712\n",
            "    011 DecoderLayer             [1, 1024, 512]       3,151,360\n",
            "       012 LayerNorm             [1, 1024, 512]           1,024\n",
            "          013 Linear             [1, 1024, 512]       5,130,000\n",
            "014 DecoderOnlyTransformer                  [1, 1024]      13,402,384\n",
            "================================================================\n",
            "Total params: 33,107,488\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Summary for DecoderOnlyTransformerMoE\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)                Input Shape         Param #\n",
            "================================================================\n",
            "       000 Embedding                  [1, 1024]       5,120,000\n",
            "001 PositionalEncoding             [1, 1024, 512]               0\n",
            "       002 LayerNorm             [1, 1024, 512]           1,024\n",
            "003 MultiheadAttention             [1, 1024, 512]       1,050,624\n",
            " 004 MaskedAttention             [1, 1024, 512]       1,050,624\n",
            "       005 LayerNorm             [1, 1024, 512]           1,024\n",
            "          006 Linear             [1, 1024, 512]           1,026\n",
            "          007 Linear             [1, 1024, 512]           1,026\n",
            " 008 NoisyTopkRouter             [1, 1024, 512]           2,052\n",
            "          009 Linear                 [627, 512]       1,050,624\n",
            "            010 ReLU                [627, 2048]               0\n",
            "          011 Linear                [627, 2048]       1,049,088\n",
            "         012 Dropout                 [627, 512]               0\n",
            "          013 Expert                 [627, 512]       2,099,712\n",
            "          014 Linear                 [397, 512]       1,050,624\n",
            "            015 ReLU                [397, 2048]               0\n",
            "          016 Linear                [397, 2048]       1,049,088\n",
            "         017 Dropout                 [397, 512]               0\n",
            "          018 Expert                 [397, 512]       2,099,712\n",
            "       019 SparseMoE             [1, 1024, 512]       4,201,476\n",
            " 020 DecoderLayerMoE             [1, 1024, 512]       5,253,124\n",
            "       021 LayerNorm             [1, 1024, 512]           1,024\n",
            "          022 Linear             [1, 1024, 512]       5,130,000\n",
            "023 DecoderOnlyTransformerMoE                  [1, 1024]      15,504,148\n",
            "================================================================\n",
            "Total params: 45,716,020\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Training the Decoder-only Transformer with MoE"
      ],
      "metadata": {
        "id": "lV4CGY2ZLo1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tiny_shakespeare dataset\n",
        "dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
        "\n",
        "# Extract the text from the dataset\n",
        "texts = dataset[\"text\"]\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 128\n",
        "nhead = 2\n",
        "num_layers = 2\n",
        "d_ff = 256\n",
        "max_seq_length = 64\n",
        "batch_size = 32\n",
        "num_epochs = 1\n",
        "learning_rate = 0.0001\n",
        "dropout = 0.2\n",
        "num_experts=4\n",
        "top_k_moe=2\n",
        "\n",
        "# Tokenize and prepare data\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.fit(texts)\n",
        "vocab_size = len(tokenizer.word_to_idx)\n",
        "\n",
        "dataset = TextDataset(texts, tokenizer, max_seq_length)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model and move to device\n",
        "model_moe = DecoderOnlyTransformerMoE(vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout, num_experts, top_k_moe).to(device)\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model_moe.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_idx[\"<PAD>\"])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model_moe.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_seq, _, _ = batch  # Unpack batch\n",
        "        input_seq = input_seq.squeeze(1).to(device)  # Move input to device and remove extra dimension\n",
        "\n",
        "        # Forward pass\n",
        "        output = model_moe(input_seq)\n",
        "\n",
        "\n",
        "        # Reshape output tensor\n",
        "        output = output[:, :-1, :].contiguous().view(-1, output.size(-1))  # Shift predictions to the left\n",
        "\n",
        "        # Shift targets to the right (original targets)\n",
        "        target_seq = input_seq[:, 1:].contiguous().view(-1)\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target_seq)\n",
        "\n",
        "        # Debugging prints\n",
        "        print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "          # Debugging prints\n",
        "          print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}\")\n",
        "          print(f\"Input sequence shape: {input_seq.shape}\")\n",
        "          print(f\"Input sequence: {input_seq.unsqueeze(1)}\")\n",
        "          print(f\"Output shape before reshape: {output.shape}\")\n",
        "          print(f\"Output shape after reshape: {output.shape}\")\n",
        "          print(f\"Target sequence shape: {target_seq.shape}\")\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "-HgiNedeLo1s",
        "outputId": "3a99dc87-b27e-403f-db85-8e90be46f2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 23845\n",
            "Loss: 10.267173767089844\n",
            "Epoch: 1, Batch: 1\n",
            "Input sequence shape: torch.Size([32, 64])\n",
            "Input sequence: tensor([[[    2,  2965,    21,  ...,   463, 23442,    61]],\n",
            "\n",
            "        [[    2,  1333,   248,  ...,    21, 19363, 21343]],\n",
            "\n",
            "        [[    2,    21,  1842,  ...,    60, 19716, 18777]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    2,  7037,   120,  ...,    21,   153,    12]],\n",
            "\n",
            "        [[    2,  1283,  6413,  ...,    98,    32,  8480]],\n",
            "\n",
            "        [[    2,  6020,  4836,  ...,   102,   114, 13213]]], device='cuda:0')\n",
            "Output shape before reshape: torch.Size([2016, 23845])\n",
            "Output shape after reshape: torch.Size([2016, 23845])\n",
            "Target sequence shape: torch.Size([2016])\n",
            "Loss: 10.222108840942383\n",
            "Loss: 10.23513412475586\n",
            "Loss: 10.209972381591797\n",
            "Loss: 10.229275703430176\n",
            "Loss: 10.179506301879883\n",
            "Loss: 10.174757957458496\n",
            "Loss: 10.168421745300293\n",
            "Loss: 10.142849922180176\n",
            "Loss: 10.140947341918945\n",
            "Loss: 10.11003303527832\n",
            "Loss: 10.123114585876465\n",
            "Loss: 10.0946044921875\n",
            "Loss: 10.094963073730469\n",
            "Loss: 10.066761016845703\n",
            "Loss: 10.07027816772461\n",
            "Loss: 10.031534194946289\n",
            "Loss: 10.031319618225098\n",
            "Loss: 10.021608352661133\n",
            "Loss: 9.998250961303711\n",
            "Loss: 9.997207641601562\n",
            "Loss: 9.9706392288208\n",
            "Loss: 9.962883949279785\n",
            "Loss: 9.89989185333252\n",
            "Loss: 9.909547805786133\n",
            "Loss: 9.908331871032715\n",
            "Loss: 9.868178367614746\n",
            "Loss: 9.895550727844238\n",
            "Loss: 9.865330696105957\n",
            "Loss: 9.848482131958008\n",
            "Loss: 9.848536491394043\n",
            "Loss: 9.827926635742188\n",
            "Loss: 9.818156242370605\n",
            "Loss: 9.788262367248535\n",
            "Loss: 9.772168159484863\n",
            "Loss: 9.741018295288086\n",
            "Loss: 9.757282257080078\n",
            "Loss: 9.691367149353027\n",
            "Loss: 9.67989444732666\n",
            "Loss: 9.650842666625977\n",
            "Loss: 9.665499687194824\n",
            "Loss: 9.661425590515137\n",
            "Loss: 9.634761810302734\n",
            "Loss: 9.600346565246582\n",
            "Loss: 9.622446060180664\n",
            "Loss: 9.608698844909668\n",
            "Loss: 9.543391227722168\n",
            "Loss: 9.528526306152344\n",
            "Loss: 9.586408615112305\n",
            "Loss: 9.543526649475098\n",
            "Loss: 9.477656364440918\n",
            "Loss: 9.477190017700195\n",
            "Loss: 9.461207389831543\n",
            "Loss: 9.488996505737305\n",
            "Loss: 9.463272094726562\n",
            "Loss: 9.445322036743164\n",
            "Loss: 9.406177520751953\n",
            "Loss: 9.421733856201172\n",
            "Loss: 9.409356117248535\n",
            "Loss: 9.407793045043945\n",
            "Loss: 9.349194526672363\n",
            "Loss: 9.341825485229492\n",
            "Loss: 9.342057228088379\n",
            "Loss: 9.346168518066406\n",
            "Loss: 9.311293601989746\n",
            "Loss: 9.32213020324707\n",
            "Loss: 9.299442291259766\n",
            "Loss: 9.281726837158203\n",
            "Loss: 9.313028335571289\n",
            "Loss: 9.252181053161621\n",
            "Loss: 9.253374099731445\n",
            "Loss: 9.236902236938477\n",
            "Loss: 9.258094787597656\n",
            "Loss: 9.182585716247559\n",
            "Loss: 9.157570838928223\n",
            "Loss: 9.164634704589844\n",
            "Loss: 9.116778373718262\n",
            "Loss: 9.156163215637207\n",
            "Loss: 9.157949447631836\n",
            "Loss: 9.127908706665039\n",
            "Loss: 9.158879280090332\n",
            "Loss: 9.05696964263916\n",
            "Loss: 9.079143524169922\n",
            "Loss: 9.087419509887695\n",
            "Loss: 9.027585983276367\n",
            "Loss: 9.025845527648926\n",
            "Loss: 9.035198211669922\n",
            "Loss: 9.03834342956543\n",
            "Loss: 9.007174491882324\n",
            "Loss: 9.016363143920898\n",
            "Loss: 9.016890525817871\n",
            "Loss: 9.02552604675293\n",
            "Loss: 8.929752349853516\n",
            "Loss: 8.979327201843262\n",
            "Loss: 8.929204940795898\n",
            "Loss: 8.924654960632324\n",
            "Loss: 8.929159164428711\n",
            "Loss: 8.886452674865723\n",
            "Loss: 8.889259338378906\n",
            "Loss: 8.871122360229492\n",
            "Loss: 8.88623332977295\n",
            "Loss: 8.884737968444824\n",
            "Loss: 8.880605697631836\n",
            "Loss: 8.867191314697266\n",
            "Loss: 8.827933311462402\n",
            "Loss: 8.798407554626465\n",
            "Loss: 8.78306770324707\n",
            "Loss: 8.808881759643555\n",
            "Loss: 8.797540664672852\n",
            "Loss: 8.76797866821289\n",
            "Loss: 8.762683868408203\n",
            "Loss: 8.744065284729004\n",
            "Loss: 8.744983673095703\n",
            "Loss: 8.71997356414795\n",
            "Loss: 8.760674476623535\n",
            "Loss: 8.682991027832031\n",
            "Loss: 8.728036880493164\n",
            "Loss: 8.623235702514648\n",
            "Loss: 8.651741981506348\n",
            "Loss: 8.714916229248047\n",
            "Loss: 8.657267570495605\n",
            "Loss: 8.676454544067383\n",
            "Loss: 8.6093111038208\n",
            "Loss: 8.635756492614746\n",
            "Loss: 8.675313949584961\n",
            "Loss: 8.632403373718262\n",
            "Loss: 8.546127319335938\n",
            "Loss: 8.619349479675293\n",
            "Loss: 8.590094566345215\n",
            "Loss: 8.645174026489258\n",
            "Loss: 8.672369956970215\n",
            "Loss: 8.545483589172363\n",
            "Loss: 8.557828903198242\n",
            "Loss: 8.567946434020996\n",
            "Loss: 8.559852600097656\n",
            "Loss: 8.565197944641113\n",
            "Loss: 8.522686004638672\n",
            "Loss: 8.48072338104248\n",
            "Loss: 8.500738143920898\n",
            "Loss: 8.472249984741211\n",
            "Loss: 8.42994499206543\n",
            "Loss: 8.498339653015137\n",
            "Loss: 8.479724884033203\n",
            "Loss: 8.425975799560547\n",
            "Loss: 8.372417449951172\n",
            "Loss: 8.413557052612305\n",
            "Loss: 8.426194190979004\n",
            "Loss: 8.421764373779297\n",
            "Loss: 8.395149230957031\n",
            "Loss: 8.39503002166748\n",
            "Loss: 8.396421432495117\n",
            "Loss: 8.383309364318848\n",
            "Loss: 8.344339370727539\n",
            "Loss: 8.382645606994629\n",
            "Loss: 8.348600387573242\n",
            "Loss: 8.316426277160645\n",
            "Loss: 8.395153045654297\n",
            "Loss: 8.30556869506836\n",
            "Loss: 8.309454917907715\n",
            "Loss: 8.296393394470215\n",
            "Loss: 8.365424156188965\n",
            "Loss: 8.291799545288086\n",
            "Loss: 8.345986366271973\n",
            "Loss: 8.286747932434082\n",
            "Loss: 8.265935897827148\n",
            "Loss: 8.264656066894531\n",
            "Loss: 8.270971298217773\n",
            "Loss: 8.38243293762207\n",
            "Loss: 8.285290718078613\n",
            "Loss: 8.22425651550293\n",
            "Loss: 8.242902755737305\n",
            "Loss: 8.192405700683594\n",
            "Loss: 8.21800708770752\n",
            "Loss: 8.21126651763916\n",
            "Loss: 8.196614265441895\n",
            "Loss: 8.13339900970459\n",
            "Loss: 8.179591178894043\n",
            "Loss: 8.206735610961914\n",
            "Loss: 8.178678512573242\n",
            "Loss: 8.131583213806152\n",
            "Loss: 8.206830978393555\n",
            "Loss: 8.21396541595459\n",
            "Loss: 8.101377487182617\n",
            "Loss: 8.182767868041992\n",
            "Loss: 8.187122344970703\n",
            "Loss: 8.151705741882324\n",
            "Loss: 8.108508110046387\n",
            "Loss: 8.125700950622559\n",
            "Loss: 8.161398887634277\n",
            "Loss: 8.108011245727539\n",
            "Loss: 8.106900215148926\n",
            "Loss: 8.112927436828613\n",
            "Loss: 8.040266990661621\n",
            "Loss: 8.082812309265137\n",
            "Loss: 8.132070541381836\n",
            "Loss: 8.05959415435791\n",
            "Loss: 8.009974479675293\n",
            "Loss: 8.069318771362305\n",
            "Loss: 8.080273628234863\n",
            "Loss: 8.00118637084961\n",
            "Loss: 8.004067420959473\n",
            "Loss: 7.9772515296936035\n",
            "Loss: 8.07552719116211\n",
            "Loss: 8.08932876586914\n",
            "Loss: 8.041569709777832\n",
            "Loss: 7.983301162719727\n",
            "Loss: 8.005146980285645\n",
            "Loss: 8.040609359741211\n",
            "Loss: 8.02466869354248\n",
            "Loss: 7.930304527282715\n",
            "Loss: 7.9626288414001465\n",
            "Loss: 7.96237850189209\n",
            "Loss: 8.028312683105469\n",
            "Loss: 7.925722599029541\n",
            "Loss: 7.886352062225342\n",
            "Loss: 7.937343597412109\n",
            "Loss: 8.01538372039795\n",
            "Loss: 7.941987991333008\n",
            "Loss: 7.873409748077393\n",
            "Loss: 8.012628555297852\n",
            "Loss: 7.916308879852295\n",
            "Loss: 7.899980545043945\n",
            "Loss: 7.8473310470581055\n",
            "Loss: 7.9552998542785645\n",
            "Loss: 8.044240951538086\n",
            "Loss: 7.8929877281188965\n",
            "Loss: 7.937827110290527\n",
            "Loss: 7.804518222808838\n",
            "Loss: 7.8410258293151855\n",
            "Loss: 7.88126802444458\n",
            "Loss: 7.8601884841918945\n",
            "Loss: 7.889366626739502\n",
            "Loss: 7.859829902648926\n",
            "Loss: 7.853671073913574\n",
            "Loss: 7.822073936462402\n",
            "Loss: 7.938673496246338\n",
            "Loss: 7.854900360107422\n",
            "Loss: 7.835086345672607\n",
            "Loss: 7.830105304718018\n",
            "Loss: 7.821161270141602\n",
            "Loss: 7.815476894378662\n",
            "Loss: 7.802312850952148\n",
            "Loss: 7.8415846824646\n",
            "Loss: 7.863821506500244\n",
            "Loss: 7.75062894821167\n",
            "Loss: 7.831700801849365\n",
            "Loss: 7.848602294921875\n",
            "Loss: 7.8139166831970215\n",
            "Loss: 7.711777210235596\n",
            "Loss: 7.840271949768066\n",
            "Loss: 7.765807628631592\n",
            "Loss: 7.769181728363037\n",
            "Loss: 7.799717426300049\n",
            "Loss: 7.741333484649658\n",
            "Loss: 7.82803201675415\n",
            "Loss: 7.683103561401367\n",
            "Loss: 7.725409507751465\n",
            "Loss: 7.801028251647949\n",
            "Loss: 7.771730899810791\n",
            "Loss: 7.757099628448486\n",
            "Loss: 7.817943096160889\n",
            "Loss: 7.784434795379639\n",
            "Loss: 7.717657089233398\n",
            "Loss: 7.786088943481445\n",
            "Loss: 7.702859878540039\n",
            "Loss: 7.786243915557861\n",
            "Loss: 7.729752540588379\n",
            "Loss: 7.693058490753174\n",
            "Loss: 7.6679253578186035\n",
            "Loss: 7.693295478820801\n",
            "Loss: 7.7218098640441895\n",
            "Loss: 7.731257438659668\n",
            "Loss: 7.692458629608154\n",
            "Loss: 7.7203497886657715\n",
            "Loss: 7.751209735870361\n",
            "Loss: 7.7615861892700195\n",
            "Loss: 7.716330051422119\n",
            "Loss: 7.751312732696533\n",
            "Loss: 7.665392875671387\n",
            "Loss: 7.6536335945129395\n",
            "Loss: 7.569649696350098\n",
            "Loss: 7.654043197631836\n",
            "Loss: 7.696451663970947\n",
            "Loss: 7.723670482635498\n",
            "Loss: 7.740908145904541\n",
            "Loss: 7.680270671844482\n",
            "Loss: 7.758261680603027\n",
            "Loss: 7.681232929229736\n",
            "Loss: 7.649751663208008\n",
            "Loss: 7.771175861358643\n",
            "Loss: 7.671345233917236\n",
            "Loss: 7.692361831665039\n",
            "Loss: 7.639220714569092\n",
            "Loss: 7.762081146240234\n",
            "Loss: 7.80443000793457\n",
            "Loss: 7.590157508850098\n",
            "Loss: 7.593641757965088\n",
            "Loss: 7.792134761810303\n",
            "Loss: 7.690086841583252\n",
            "Loss: 7.724594593048096\n",
            "Loss: 7.548190116882324\n",
            "Loss: 7.605767250061035\n",
            "Loss: 7.645823001861572\n",
            "Loss: 7.652649402618408\n",
            "Loss: 7.649279594421387\n",
            "Loss: 7.675906181335449\n",
            "Loss: 7.673112392425537\n",
            "Loss: 7.638182163238525\n",
            "Loss: 7.704764366149902\n",
            "Loss: 7.702300548553467\n",
            "Loss: 7.637600421905518\n",
            "Loss: 7.631369590759277\n",
            "Loss: 7.595473289489746\n",
            "Loss: 7.612586975097656\n",
            "Loss: 7.59210205078125\n",
            "Loss: 7.771147727966309\n",
            "Loss: 7.664365768432617\n",
            "Loss: 7.7068891525268555\n",
            "Loss: 7.647665500640869\n",
            "Loss: 7.66152811050415\n",
            "Loss: 7.626660346984863\n",
            "Loss: 7.655455589294434\n",
            "Loss: 7.654713153839111\n",
            "Loss: 7.7101311683654785\n",
            "Loss: 7.581860065460205\n",
            "Loss: 7.570280075073242\n",
            "Loss: 7.707910060882568\n",
            "Loss: 7.632372856140137\n",
            "Loss: 7.685187816619873\n",
            "Loss: 7.670289516448975\n",
            "Loss: 7.592286586761475\n",
            "Loss: 7.655519485473633\n",
            "Loss: 7.656875133514404\n",
            "Loss: 7.724973201751709\n",
            "Loss: 7.687918663024902\n",
            "Loss: 7.591799259185791\n",
            "Loss: 7.619603633880615\n",
            "Loss: 7.648255348205566\n",
            "Loss: 7.740840911865234\n",
            "Loss: 7.680533409118652\n",
            "Loss: 7.719671726226807\n",
            "Loss: 7.709496021270752\n",
            "Loss: 7.584148406982422\n",
            "Loss: 7.59222936630249\n",
            "Loss: 7.591519355773926\n",
            "Loss: 7.6083478927612305\n",
            "Loss: 7.658524990081787\n",
            "Loss: 7.638701438903809\n",
            "Loss: 7.648947715759277\n",
            "Loss: 7.576687812805176\n",
            "Loss: 7.61597204208374\n",
            "Loss: 7.6366682052612305\n",
            "Loss: 7.674760818481445\n",
            "Loss: 7.720759868621826\n",
            "Loss: 7.596978664398193\n",
            "Loss: 7.597512722015381\n",
            "Loss: 7.610846519470215\n",
            "Loss: 7.592689037322998\n",
            "Loss: 7.72462272644043\n",
            "Loss: 7.66123104095459\n",
            "Loss: 7.68264627456665\n",
            "Loss: 7.681153297424316\n",
            "Loss: 7.654641151428223\n",
            "Loss: 7.604292869567871\n",
            "Loss: 7.64767599105835\n",
            "Loss: 7.594171047210693\n",
            "Loss: 7.680426120758057\n",
            "Loss: 7.639978885650635\n",
            "Loss: 7.553709506988525\n",
            "Loss: 7.570774555206299\n",
            "Loss: 7.565342903137207\n",
            "Loss: 7.629770755767822\n",
            "Loss: 7.688697338104248\n",
            "Loss: 7.627687454223633\n",
            "Loss: 7.517200469970703\n",
            "Loss: 7.629452228546143\n",
            "Loss: 7.6146345138549805\n",
            "Loss: 7.677741050720215\n",
            "Loss: 7.700375556945801\n",
            "Loss: 7.580137729644775\n",
            "Loss: 7.494729042053223\n",
            "Loss: 7.608257293701172\n",
            "Loss: 7.603484153747559\n",
            "Loss: 7.639840126037598\n",
            "Loss: 7.638676166534424\n",
            "Loss: 7.563831329345703\n",
            "Loss: 7.626943588256836\n",
            "Loss: 7.64601993560791\n",
            "Loss: 7.7421698570251465\n",
            "Loss: 7.656230449676514\n",
            "Loss: 7.526761531829834\n",
            "Loss: 7.6369757652282715\n",
            "Loss: 7.6246466636657715\n",
            "Loss: 7.559882164001465\n",
            "Loss: 7.667392253875732\n",
            "Loss: 7.571407318115234\n",
            "Loss: 7.677426338195801\n",
            "Loss: 7.582180023193359\n",
            "Loss: 7.726297855377197\n",
            "Loss: 7.57438325881958\n",
            "Loss: 7.600820541381836\n",
            "Loss: 7.619383335113525\n",
            "Loss: 7.594427585601807\n",
            "Loss: 7.589508533477783\n",
            "Loss: 7.670905590057373\n",
            "Loss: 7.571263790130615\n",
            "Loss: 7.687952995300293\n",
            "Loss: 7.54531717300415\n",
            "Loss: 7.6056599617004395\n",
            "Loss: 7.708146572113037\n",
            "Loss: 7.622893810272217\n",
            "Loss: 7.666560173034668\n",
            "Loss: 7.608297348022461\n",
            "Loss: 7.63621187210083\n",
            "Loss: 7.595677852630615\n",
            "Loss: 7.595763683319092\n",
            "Loss: 7.612528324127197\n",
            "Loss: 7.5812554359436035\n",
            "Loss: 7.59243106842041\n",
            "Loss: 7.576923847198486\n",
            "Loss: 7.540746212005615\n",
            "Loss: 7.529106616973877\n",
            "Loss: 7.691413402557373\n",
            "Loss: 7.561331748962402\n",
            "Loss: 7.598711013793945\n",
            "Loss: 7.687596321105957\n",
            "Loss: 7.583075046539307\n",
            "Loss: 7.548548221588135\n",
            "Loss: 7.657137870788574\n",
            "Loss: 7.687612056732178\n",
            "Loss: 7.688837051391602\n",
            "Loss: 7.652298927307129\n",
            "Loss: 7.61853551864624\n",
            "Loss: 7.6374921798706055\n",
            "Loss: 7.6413469314575195\n",
            "Loss: 7.55489444732666\n",
            "Loss: 7.604781627655029\n",
            "Loss: 7.540354251861572\n",
            "Loss: 7.622880935668945\n",
            "Epoch 1/1, Loss: 8.2890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍ Testing the Decoder-only Transformer with MoE"
      ],
      "metadata": {
        "id": "Ll7zTpyWLo1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Better three hours too soon than\", \" I believe I can \", \"My words fly up, my\", \"Brevity is \", \"Love looks not with the eyes, but\", \"To be or \"]\n",
        "\n",
        "for quote in texts:\n",
        "  start_tokens = torch.tensor(tokenizer.encode(quote)).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "  generated_tokens = model_moe.generate(start_tokens, max_length=20, temperature=.9)\n",
        "  generated_text = tokenizer.decode(generated_tokens.squeeze().tolist())\n",
        "\n",
        "  print(generated_text)"
      ],
      "metadata": {
        "id": "yDzYIElYLo1s",
        "outputId": "65b3e2ab-ce72-49ff-b3f3-6e93370cf3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Better three hours too soon than so: mark to they the My than I her, for shall good the are\n",
            "I believe I can I this crowned, for like my the of Fifth we him deserved of beat to him,\n",
            "My words fly up, my Be and them, a Her O, That point for after me his my a take\n",
            "<UNK> is the Margaret: apparent? putter-on of the she But broad-spreading here DUKE of is enough! yours; Fortunate your SAMPSON:\n",
            "Love looks not with the eyes, but eat. Peter, of thy will of joys thou From Unto pulled more. you\n",
            "To be or to feel than prepared I KING will and and glasses does your penitence: fly: usual the stuff'd,\n"
          ]
        }
      ]
    }
  ]
}